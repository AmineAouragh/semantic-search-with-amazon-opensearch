{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "We will use the semantic search to provide the best matching wine based on the review description. [Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401) is a process that combines retrieval-based models and generative models to enhance natural language generation by retrieving relevant information and incorporating it into the generation process. In this notebook, we'll walk through enhancing an OpenSearch cluster search with generative AI to output conversational wine recommendations based on a desired description."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "571c4d1e",
   "metadata": {},
   "source": [
    "### 1. Install OpenSearch ML Python library\n",
    "\n",
    "For this notebook we require the use of a few key libraries. We'll use the Python clients for OpenSearch and SageMaker, and Python frameworks for text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opensearch-py-ml accelerate sentence-transformers tqdm --quiet\n",
    "!pip install sagemaker --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31703e3d",
   "metadata": {},
   "source": [
    "### 2. Check PyTorch Version\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ac12126",
   "metadata": {},
   "source": [
    "As in the previous modules, let's import PyTorch and confirm that the latest version of PyTorch is running. The version should already be 1.13.1 or higher. If not, please run the lab in order to get everything set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b532987",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2f1cc51",
   "metadata": {},
   "source": [
    "### 3. Retrieve notebook variables\n",
    "\n",
    "The line below will retrieve your shared variables from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0e06e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05c00375",
   "metadata": {},
   "source": [
    "Now we need to restart the kernel by running below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94df946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa614bc",
   "metadata": {},
   "source": [
    "### 4. Import libraries\n",
    "The line below will import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from transformers import AutoTokenizer, AutoModel, DistilBertTokenizer, DistilBertModel\n",
    "from sagemaker import get_execution_role, image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6607721",
   "metadata": {},
   "source": [
    "### 5. Prepare data\n",
    "\n",
    "This lab combines semantic search with a generative model to present the retrieved data to the user in a conversational tone. Below is a dataset of wine reviews, we'll sample this data set to recommend wines that resemble the user provided description.\n",
    "\n",
    "### Note\n",
    "You can download the dataset from various sources. One is Kaggle.\n",
    "https://www.kaggle.com/datasets/christopheiv/winemagdata130k?select=winemag-data-130k-v2.json\n",
    "\n",
    "After downloading and copying here, unzip in the working directory if it hasn't already been unzipped. Execute the following cells to inspect the dataset, transform it into a pandas DataFrame, and sample a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fca957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!unzip -o winemag-data-130k-v2.json.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a462a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('winemag-data-130k-v2.json')\n",
    "\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1cf47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wm_list = df.sample(300,\n",
    "                   random_state=37).to_dict('records') # sample to keep lab quick\n",
    "\n",
    "wm_list[:1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f54a349",
   "metadata": {},
   "source": [
    "### 6. Create an OpenSearch cluster connection.\n",
    "Next, we'll use Python API to set up connection with OpenSearch Cluster.\n",
    "\n",
    "Note: if you're using a region other than us-east-1, please update the region in the code below.\n",
    "\n",
    "#### Get Cloud Formation stack output variables\n",
    "\n",
    "We also need to grab some key values from the infrastructure we provisioned using CloudFormation. To do this, we will list the outputs from the stack and store this in \"outputs\" to be used later.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = 'us-east-1' \n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auth = (\"master\",\"Semantic123!\")\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba0742cd",
   "metadata": {},
   "source": [
    "### 7. Download and Deploy Text Embeddings Endpoint\n",
    "\n",
    "Similar to the previous modules, we will be using the BERT model to generate vectorization data, where every sentence is 768 dimension data. As shown in Modules 3 and 4, embedding text allows us to search the cluster more accurately than the default text matching. \n",
    "\n",
    "We will be downloading the pre-trained embeddings model and deploying it as a SageMaker endpoint for inference. Deploying the model may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\"\n",
    "saved_model_dir = 'transformer'\n",
    "os.makedirs(saved_model_dir, exist_ok=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name) \n",
    "\n",
    "tokenizer.save_pretrained(saved_model_dir)\n",
    "model.save_pretrained(saved_model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d497ac9e",
   "metadata": {},
   "source": [
    "Create a compressed model file `model.tar.gz` in the parent directory from the downloaded model so that we can deploy the model to an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a202a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd transformer && tar czvf ../model.tar.gz *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d37bd888",
   "metadata": {},
   "source": [
    "Upload `model.tar.gz` and deploy an inference endpoint to embed text. This cell may take a few minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringPredictor(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='sentence-transformers-model')\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data = inputs, \n",
    "                             role=role, \n",
    "                             entry_point ='inference.py',\n",
    "                             source_dir = './code',\n",
    "                             py_version = 'py39', \n",
    "                             framework_version = '1.13.1',\n",
    "                             predictor_cls=StringPredictor)\n",
    "\n",
    "\n",
    "\n",
    "embed_predictor = pytorch_model.deploy(instance_type='ml.m5d.large', \n",
    "                                 initial_instance_count=1, \n",
    "                                 endpoint_name = f'distilbert-embedding-model-{int(time.time())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86c3e1ae",
   "metadata": {},
   "source": [
    "If you already deployed a model, skip the execution of the previous cell, uncomment the below cell, and add your endpoint name to `embed_endpoint_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732c18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_endpoint_name = \"endpoint_name\"\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# embed_predictor = StringPredictor(endpoint_name=embed_endpoint_name, \n",
    "#                                   sagemaker_session = sagemaker_session)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37d66036",
   "metadata": {},
   "source": [
    "### 8. Test the embeddings endpoint with a sample phrase\n",
    "Using any text phrase, the endpoint converts the text to a vectorized array of size 768. We're also creating a function `embed_phrase` so that we can call it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_phrase(phrase):\n",
    "    features = embed_predictor.predict(phrase)\n",
    "    return json.loads(features)\n",
    "\n",
    "embed_phrase(\"pairs well with chocolate\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beaabc1e",
   "metadata": {},
   "source": [
    "### 9. Create a index in Amazon Opensearch Service \n",
    "Whereas we previously created an index with 2-3 fields, this time we'll define the index with multiple fields: the vectorization of the `description` field, and all others present within the dataset.\n",
    "\n",
    "To create the index, we first define the index in JSON, then use the aos_client connection we initiated ealier to create the index in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"designation\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"variety\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"country\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"winery\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"points\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835b9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aos_client.indices.delete(index=\"nlp_pqa\") # drop the index from the previous lab\n",
    "# If this is the first time you're running this, you won't have this index to drop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e84514d",
   "metadata": {},
   "source": [
    "Using the above index definition, we now need to create the index in Amazon OpenSearch. Running this cell will recreate the index if you have already executed this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b0ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_name = \"nlp_wmd\"\n",
    "\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(\"Recreating index '\" + index_name + \"' on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7007735",
   "metadata": {},
   "source": [
    "Let's verify the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71659d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=\"nlp_wmd\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0040992c",
   "metadata": {},
   "source": [
    "### 10. Load the raw data into the Index\n",
    "Next, let's load the wine review data into the index we've just created. During ingest data defined by the `os_import` function, `description` field will also be converted to vector (embedding) by calling the previously created endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55e6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def os_import(record, aos_client, index_name):\n",
    "    description = record[\"description\"]\n",
    "    search_vector = embed_phrase(description)\n",
    "    aos_client.index(index=index_name,\n",
    "             body={\"description_vector\": search_vector, \n",
    "                   \"description\": record[\"description\"],\n",
    "                   \"points\":record[\"points\"],\n",
    "                   \"variety\":record[\"variety\"],\n",
    "                   \"country\":record[\"country\"],\n",
    "                   \"designation\":record[\"designation\"],\n",
    "                   \"winery\":record[\"winery\"]\n",
    "                  }\n",
    "            )\n",
    "\n",
    "print(\"Index created. Loading records...\")\n",
    "for record in tqdm(wm_list): \n",
    "    os_import(record, aos_client, index_name)\n",
    "print(\"Records loaded.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67fad674",
   "metadata": {},
   "source": [
    "To validate the load, we'll query the number of documents number in the index. We should have 300 hits in the index, or however many was specified earlier in sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de9b827c",
   "metadata": {},
   "source": [
    "### 11. Search vector with \"Semantic Search\" \n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a wine whose review most closely matches the requested description. `query_opensearch` embeds the search phrase, searches the cluster index for the closest matching vector, and returns the top result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_opensearch(phrase, n=1):\n",
    "    search_vector = embed_phrase(phrase)\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"description_vector\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"description_vector\": {\n",
    "            \"vector\":search_vector,\n",
    "            \"k\":n\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=\"nlp_wmd\", \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"description\",\"winery\",\"points\", \"designation\", \"country\"],\n",
    "                           explain = True)\n",
    "    top_result = res['hits']['hits'][0]\n",
    "    \n",
    "    result = {\n",
    "        \"description\":top_result['_source']['description'],\n",
    "        \"winery\":top_result['_source']['winery'],\n",
    "        \"points\":top_result['_source']['points'],\n",
    "        \"designation\":top_result['_source']['designation'],\n",
    "        \"country\":top_result['_source']['country'],\n",
    "        \"variety\":top_result['_source']['variety'],\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "example_request = query_opensearch(\"A wine that pairs well with meat.\")\n",
    "print(example_request)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0abddaa4",
   "metadata": {},
   "source": [
    "### 12. Deploy the Large Language Model for Retrieval Augmented Generation\n",
    "\n",
    "This module uses the [Falcon 7B](https://falconllm.tii.ae) model to create recommendations based on a given wine review. The next cell deploys a model endpoint into your environment that will be called by subsequent steps. For more information on the Falcon LLM, see [HuggingFace's announcement](https://huggingface.co/blog/falcon) regarding the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e59c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = get_huggingface_llm_image_uri(\n",
    "  backend=\"huggingface\", # or lmi\n",
    "  region=region\n",
    ")\n",
    "\n",
    "model_name = \"falcon-7b-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'tiiuae/falcon-7b',\n",
    "    'HF_TASK':'question-answering',\n",
    "    'SM_NUM_GPUS':'1',\n",
    "    'HF_MODEL_QUANTIZE':'bitsandbytes'\n",
    "}\n",
    "\n",
    "model = HuggingFaceModel(\n",
    "    name=model_name,\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    image_uri=image_uri\n",
    ")\n",
    "\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "#Other supported 1-GPU instance types: ml.g5.2xlarge, ml.g5.xlarge\n",
    "#With instances with more GPUs, change the SM_NUM_GPUS value within the hub variable\n",
    "#For example, using endpoint instance ml.g5.12xlarge, specify 'SM_NUM_GPUS':'4'\n",
    "\n",
    "print(\"☕ Spinning up the endpoint. This will take a little while ☕\")\n",
    "\n",
    "llm_predictor = model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  endpoint_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a17d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already deployed a model, \n",
    "# uncomment the following lines and add your endpoint name below\n",
    "\n",
    "# from sagemaker.huggingface import HuggingFacePredictor\n",
    "# sagemaker_session = sagemaker.Session()\n",
    "# llm_endpoint_name = \"endpoint_name\"\n",
    "# llm_predictor = HuggingFacePredictor(endpoint_name=llm_endpoint_name, sagemaker_session = sagemaker_session)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6fa5564",
   "metadata": {},
   "source": [
    "### 13. Create a prompt for the LLM using the search results from OpenSearch\n",
    "\n",
    "We will be using the Falcon-7B model for single-shot generation, using a canned recommendation and response to guide the output. Before querying the model, the below function `render_prompt` is used to easily make a prompt for single-shot generation. The function takes in an input string to search the OpenSearch cluster for a matching wine, and outputs a viable prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prompt(requested_description):\n",
    "    recommendation = query_opensearch(requested_description)\n",
    "    sample_recommendation = \"{{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}}\"\n",
    "    sample_response = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "    prompt = (\n",
    "        f\"[CLM] Context: A sommelier uses their vast knowledge of wine to make great recommendations people will enjoy. A recommendation always includes the winery, the country of origin, and a colorful description.\"\n",
    "        f\"Data: <br> Recommendation: <br>\"\n",
    "        f\"Data: {sample_recommendation} <br> Recommendation: {sample_response} <br>\"\n",
    "        f\"Data: {recommendation} <br> Recommendation:\"\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "prompt = render_prompt(\"pairs well with meat\") \n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c26aa75c",
   "metadata": {},
   "source": [
    "### 14. Format LLM input and query using the rendered prompt\n",
    "We also need a few more helper functions to query the LLM. `render_llm_input` transforms the generated prompt into the correct input format, `render_llm_output` parses the LLM output. \n",
    "\n",
    "`query_llm` combines everything we've done in this module. It does all of the following:\n",
    "- embeds the input (the desired description of a wine)\n",
    "- searches the OpenSearch index for the closest description vector\n",
    "- renders an LLM prompt from the search results\n",
    "- queries the LLM for a conversational response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_llm_input(data, **kwargs):\n",
    "    default_kwargs = {\n",
    "        \"num_beams\": 5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"temperature\": 0.6,\n",
    "        \"watermark\": True,\n",
    "        \"top_k\": 147,\n",
    "        \"max_length\": 175,\n",
    "        \"early_stopping\": True\n",
    "    }\n",
    "    \n",
    "    default_kwargs = {**default_kwargs, **kwargs}\n",
    "    \n",
    "    input_data = {\n",
    "        \"inputs\": data,\n",
    "        \"parameters\": default_kwargs\n",
    "    }\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "def render_llm_output(response):\n",
    "    return response.split(\"Recommendation: \")[-1].split(\"<br>\")[0].split(\"Data:\")[0].split(\"\\n\")[0]\n",
    "\n",
    "def query_llm(description, **kwargs):\n",
    "    prompt = render_prompt(description)\n",
    "    query = render_llm_input(prompt, **kwargs)\n",
    "    response = llm_predictor.predict(query)[0]['generated_text']\n",
    "    rec = render_llm_output(response)\n",
    "    return rec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62de7273",
   "metadata": {},
   "source": [
    "#### And finally, let's call the function and get a wine recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation = query_llm(\"A wine that pairs well with meat.\")\n",
    "print(recommendation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2adc9a2",
   "metadata": {},
   "source": [
    "### Additional info: changing kwargs for querying the LLM\n",
    "If you want to change or add new parameters for LLM querying, you're able to add in new keyword arguments to the `query_llm` function. For example, to change the `temperature` value, simply change the function call:\n",
    "`query_llm(description phrase, temperature = new float value)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f10a4bf",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "After you've finished using the endpoints, it's important to delete it to avoid incurring unnecessary costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b95cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_predictor.delete_endpoint()\n",
    "llm_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
