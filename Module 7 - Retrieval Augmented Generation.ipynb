{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "We will use the semantic search to provide the best matching wine based on the review description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31703e3d",
   "metadata": {},
   "source": [
    "### 1. Check PyTorch Version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac12126",
   "metadata": {},
   "source": [
    "As in the previous modules, let's import PyTorch and confirm that have have the latest version of PyTorch. The version should already be 1.13.1 or higher. If not, please run the lab in order to get everything set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b532987",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1cc51",
   "metadata": {},
   "source": [
    "### 2. Retrieve notebook variables\n",
    "\n",
    "The line below will retrieve your shared variables from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a0e06e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3fa4b0",
   "metadata": {},
   "source": [
    "### 3. Install OpenSearch ML Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a1c491",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting opensearch-py-ml\n",
      "  Downloading opensearch_py_ml-1.0.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<2,>=1.24.0\n",
      "  Downloading numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib<4,>=3.6.0\n",
      "  Downloading matplotlib-3.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas<2,>=1.5\n",
      "  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opensearch-py>=2\n",
      "  Downloading opensearch_py-2.2.0-py2.py3-none-any.whl (291 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources>=3.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib<4,>=3.6.0->opensearch-py-ml) (5.10.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib<4,>=3.6.0->opensearch-py-ml) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib<4,>=3.6.0->opensearch-py-ml) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib<4,>=3.6.0->opensearch-py-ml) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib<4,>=3.6.0->opensearch-py-ml) (1.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib<4,>=3.6.0->opensearch-py-ml) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib<4,>=3.6.0->opensearch-py-ml) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib<4,>=3.6.0->opensearch-py-ml) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from matplotlib<4,>=3.6.0->opensearch-py-ml) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from opensearch-py>=2->opensearch-py-ml) (2.28.1)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from opensearch-py>=2->opensearch-py-ml) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from opensearch-py>=2->opensearch-py-ml) (2022.12.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from opensearch-py>=2->opensearch-py-ml) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas<2,>=1.5->opensearch-py-ml) (2022.7)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib<4,>=3.6.0->opensearch-py-ml) (3.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py>=2->opensearch-py-ml) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py>=2->opensearch-py-ml) (2.1.1)\n",
      "Installing collected packages: numpy, pandas, opensearch-py, matplotlib, opensearch-py-ml\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.4\n",
      "    Uninstalling pandas-1.4.4:\n",
      "      Successfully uninstalled pandas-1.4.4\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.3\n",
      "    Uninstalling matplotlib-3.5.3:\n",
      "      Successfully uninstalled matplotlib-3.5.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed matplotlib-3.7.1 numpy-1.24.3 opensearch-py-2.2.0 opensearch-py-ml-1.0.0 pandas-1.5.3\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torch>=1.4.0->accelerate) (4.4.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.18.0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (4.63.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (1.0)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (1.8.1)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers) (3.8.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2022.11.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchvision->sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.8)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=695ff774274ad7e43f9678e10cfdc9841397aa4628f7e250ab1b708ae86ba452\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4b/68/65/aba8be86302d9988b832f5e1f3417a87e4a868d396e4329f0a\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.14.1 sentence-transformers-2.2.2 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.28.1\n"
     ]
    }
   ],
   "source": [
    "!pip install opensearch-py-ml\n",
    "!pip install accelerate\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c00375",
   "metadata": {},
   "source": [
    "Now we need to restart the kernel by running below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94df946",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa614bc",
   "metadata": {},
   "source": [
    "### 4. Import library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6607721",
   "metadata": {},
   "source": [
    "### 5. Prepare data\n",
    "\n",
    "This lab combines semantic search with a generative model to present the retrieved data to the user in a conversational tone. This is a dataset of wine reviews. We'll sample this data set to recomend wines that resemble the user provided description.\n",
    "\n",
    "### Note\n",
    "You can download the dataset from various sources. One is Kaggle.\n",
    "https://www.kaggle.com/datasets/christopheiv/winemagdata130k?select=winemag-data-130k-v2.json\n",
    "\n",
    "After downloading and copying here, unzip in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fca957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!unzip -o winemag-data-130k-v2.json.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4a462a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>points</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>price</th>\n",
       "      <th>designation</th>\n",
       "      <th>variety</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>province</th>\n",
       "      <th>country</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49635</th>\n",
       "      <td>80</td>\n",
       "      <td>Ca' Momi 2007 Rosso Di California Red (Califor...</td>\n",
       "      <td>Soft and syrupy, with semisweet flavors of ber...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Rosso Di California</td>\n",
       "      <td>Red Blend</td>\n",
       "      <td>California</td>\n",
       "      <td>California Other</td>\n",
       "      <td>California</td>\n",
       "      <td>US</td>\n",
       "      <td>Ca' Momi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89344</th>\n",
       "      <td>86</td>\n",
       "      <td>Geografico Castello Tricerchi 2008  Rosso di M...</td>\n",
       "      <td>Light and luminous in appearance, this bright ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>Sangiovese Grosso</td>\n",
       "      <td>Rosso di Montalcino</td>\n",
       "      <td>None</td>\n",
       "      <td>Tuscany</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Geografico Castello Tricerchi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25158</th>\n",
       "      <td>88</td>\n",
       "      <td>Dr. Pauly Bergweiler 2012 Wehlener Klosterberg...</td>\n",
       "      <td>A veritable kaleidoscope of scents, this off-d...</td>\n",
       "      <td>Anna Lee C. Iijima</td>\n",
       "      <td>None</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Wehlener Klosterberg</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mosel</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Dr. Pauly Bergweiler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       points                                              title  \\\n",
       "49635      80  Ca' Momi 2007 Rosso Di California Red (Califor...   \n",
       "89344      86  Geografico Castello Tricerchi 2008  Rosso di M...   \n",
       "25158      88  Dr. Pauly Bergweiler 2012 Wehlener Klosterberg...   \n",
       "\n",
       "                                             description         taster_name  \\\n",
       "49635  Soft and syrupy, with semisweet flavors of ber...                None   \n",
       "89344  Light and luminous in appearance, this bright ...                None   \n",
       "25158  A veritable kaleidoscope of scents, this off-d...  Anna Lee C. Iijima   \n",
       "\n",
       "      taster_twitter_handle  price           designation            variety  \\\n",
       "49635                  None   10.0   Rosso Di California          Red Blend   \n",
       "89344                  None    NaN                  None  Sangiovese Grosso   \n",
       "25158                  None   17.0  Wehlener Klosterberg           Riesling   \n",
       "\n",
       "                  region_1          region_2    province  country  \\\n",
       "49635           California  California Other  California       US   \n",
       "89344  Rosso di Montalcino              None     Tuscany    Italy   \n",
       "25158                 None              None       Mosel  Germany   \n",
       "\n",
       "                              winery  \n",
       "49635                       Ca' Momi  \n",
       "89344  Geografico Castello Tricerchi  \n",
       "25158           Dr. Pauly Bergweiler  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('winemag-data-130k-v2.json')\n",
    "\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854c4aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['points', 'title', 'description', 'taster_name',\n",
       "       'taster_twitter_handle', 'price', 'designation', 'variety', 'region_1',\n",
       "       'region_2', 'province', 'country', 'winery'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1cf47b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'points': 89,\n",
       "  'title': 'Calera 2014 Pinot Noir (Central Coast)',\n",
       "  'description': \"Relatively light in the glass, this blend of nine vineyards from up and down the coast shows savory baked plum, cardamom dust, earthy cranberry, bay leaf and smoked meat on the nose. It's medium bodied on the palate and very down-the-middle flavor-wise, with cranberry, oregano and thyme elements.\",\n",
       "  'taster_name': 'Matt Kettmann',\n",
       "  'taster_twitter_handle': '@mattkettmann',\n",
       "  'price': 28.0,\n",
       "  'designation': None,\n",
       "  'variety': 'Pinot Noir',\n",
       "  'region_1': 'Central Coast',\n",
       "  'region_2': 'Central Coast',\n",
       "  'province': 'California',\n",
       "  'country': 'US',\n",
       "  'winery': 'Calera'},\n",
       " {'points': 92,\n",
       "  'title': 'Wallis Family Estate 2008 Little Sister Red (Diamond Mountain District)',\n",
       "  'description': 'Dry and smooth as a fine old Scotch, with intricately woven blackberry, black currant, green olive and herb flavors leading to a long, spicy finish. The tannins are significant, but very finely ground. Classy and powerful, this Cabernet should develop over the next eight years.',\n",
       "  'taster_name': None,\n",
       "  'taster_twitter_handle': None,\n",
       "  'price': 45.0,\n",
       "  'designation': 'Little Sister',\n",
       "  'variety': 'Bordeaux-style Red Blend',\n",
       "  'region_1': 'Diamond Mountain District',\n",
       "  'region_2': 'Napa',\n",
       "  'province': 'California',\n",
       "  'country': 'US',\n",
       "  'winery': 'Wallis Family Estate'},\n",
       " {'points': 84,\n",
       "  'title': 'Tucumen 2012 Cabernet Sauvignon (Mendoza)',\n",
       "  'description': 'Pointy cherry aromas initially suggest nail polish and volatility. A gritty palate with candied red-fruit flavors seems generic, while berry, chocolate and marshmallow flavors lend sweetness to the finish.',\n",
       "  'taster_name': 'Michael Schachner',\n",
       "  'taster_twitter_handle': '@wineschach',\n",
       "  'price': 16.0,\n",
       "  'designation': None,\n",
       "  'variety': 'Cabernet Sauvignon',\n",
       "  'region_1': 'Mendoza',\n",
       "  'region_2': None,\n",
       "  'province': 'Mendoza Province',\n",
       "  'country': 'Argentina',\n",
       "  'winery': 'Tucumen'},\n",
       " {'points': 90,\n",
       "  'title': 'Giant Steps 2013 Applejack Vineyard Pinot Noir (Yarra Valley)',\n",
       "  'description': \"Located in the upper Yarra, this site normally yields Pinots higher in acid and tighter in structure than the Sexton Vineyard. The 2013 marries fruity and savory elements of cherries and mushrooms in a medium-bodied, silky package. It's bright and generous from the bold start to the lengthy finish. Drink now–2023.\",\n",
       "  'taster_name': 'Joe Czerwinski',\n",
       "  'taster_twitter_handle': '@JoeCz',\n",
       "  'price': 42.0,\n",
       "  'designation': 'Applejack Vineyard',\n",
       "  'variety': 'Pinot Noir',\n",
       "  'region_1': 'Yarra Valley',\n",
       "  'region_2': None,\n",
       "  'province': 'Victoria',\n",
       "  'country': 'Australia',\n",
       "  'winery': 'Giant Steps'},\n",
       " {'points': 87,\n",
       "  'title': 'Lenné Estate 2008 Pinot Noir',\n",
       "  'description': 'Held back and re-released in the spring of 2014, this is smooth and still fruity, the fruit rounding into secondary flavors. Not a big wine, it smoothes out quickly and seems to have a hint of leather, with the finish tailing into a vague fade.',\n",
       "  'taster_name': 'Paul Gregutt',\n",
       "  'taster_twitter_handle': '@paulgwine\\xa0',\n",
       "  'price': 65.0,\n",
       "  'designation': None,\n",
       "  'variety': 'Pinot Noir',\n",
       "  'region_1': 'Willamette Valley',\n",
       "  'region_2': None,\n",
       "  'province': 'Oregon',\n",
       "  'country': 'US',\n",
       "  'winery': 'Lenné Estate'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# wm_list = df.to_dict('records')\n",
    "wm_list = df.sample(300,\n",
    "                   random_state=37).to_dict('records') # sample to keep lab quick\n",
    "\n",
    "wm_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54a349",
   "metadata": {},
   "source": [
    "### 6. Create an OpenSearch cluster connection.\n",
    "Next, we'll use Python API to set up connection with OpenSearch Cluster.\n",
    "\n",
    "Note: if you're using a region other than us-east-1, please update the region in the code below.\n",
    "\n",
    "#### Get Cloud Formation stack output variables\n",
    "\n",
    "We also need to grab some key values from the infrastructure we provisioned using CloudFormation. To do this, we will list the outputs from the stack and store this in \"outputs\" to be used later.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OpenSourceDomainArn': 'arn:aws:es:us-east-1:935924611070:domain/opensearchservi-d3f0c7mvypbf',\n",
       " 'OpenSearchDomainEndpoint': 'search-opensearchservi-d3f0c7mvypbf-qki7nvr5tkt2bngllgdii2eh2e.us-east-1.es.amazonaws.com',\n",
       " 'S3BucketSecureURL': 'https://semantic-search-s3buckethosting-nv48itz8gn94.s3.amazonaws.com',\n",
       " 'SageMakerNotebookURL': 'https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances/openNotebook/NotebookInstance-jcHRY2YDGdBn?view=classic',\n",
       " 's3BucketTraining': 'semantic-search-s3buckettraining-ht47aafjizwe',\n",
       " 'Region': 'us-east-1',\n",
       " 'OpenSearchDomainName': 'opensearchservi-d3f0c7mvypbf',\n",
       " 's3BucketHostingBucketName': 'semantic-search-s3buckethosting-nv48itz8gn94'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "bucket = outputs['s3BucketTraining']\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "\n",
    "region = 'us-east-1' \n",
    "\n",
    "#credentials = boto3.Session().get_credentials()\n",
    "#auth = AWSV4SignerAuth(credentials, region)\n",
    "auth = (\"master\",\"Semantic123!\")\n",
    "index_name = 'nlp_wmd'\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173ff23",
   "metadata": {},
   "source": [
    "### 7. Configure OpenSearch domain to enable run Machine Learning code in data node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4080d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'persistent': {},\n",
       " 'transient': {'plugins': {'ml_commons': {'only_run_on_ml_node': 'false'}}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = b'{\"transient\":{\"plugins.ml_commons.only_run_on_ml_node\": false}}'\n",
    "aos_client.cluster.put_settings(body=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a9cd9",
   "metadata": {},
   "source": [
    "Verify `plugins.ml_commons.only_run_on_ml_node` is set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce6a646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persistent': {'cluster.routing.allocation.awareness.force.zone.values': 'xx-xxxxx-xx',\n",
       "  'cluster.routing.allocation.disk.watermark.flood_stage': '0.9758094787597656gb',\n",
       "  'cluster.routing.allocation.disk.watermark.high': '1.9516189575195313gb',\n",
       "  'cluster.routing.allocation.disk.watermark.low': '2.9274284362792966gb',\n",
       "  'cluster.routing.allocation.load_awareness.provisioned_capacity': '1',\n",
       "  'cluster.routing.allocation.load_awareness.skew_factor': '50.0',\n",
       "  'cluster_manager.throttling.thresholds.auto-create.value': '200',\n",
       "  'cluster_manager.throttling.thresholds.cluster-reroute-api.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.cluster-update-settings.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.create-component-template.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.create-data-stream.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.create-index-template-v2.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.create-index-template.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.create-index.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.create-persistent-task.value': '200',\n",
       "  'cluster_manager.throttling.thresholds.create-snapshot.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.delete-dangling-index.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.delete-index.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.delete-pipeline.value': '200',\n",
       "  'cluster_manager.throttling.thresholds.delete-repository.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.delete-script.value': '200',\n",
       "  'cluster_manager.throttling.thresholds.delete-snapshot.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.finish-persistent-task.value': '200',\n",
       "  'cluster_manager.throttling.thresholds.index-aliases.value': '200',\n",
       "  'cluster_manager.throttling.thresholds.put-mapping.value': '10000',\n",
       "  'cluster_manager.throttling.thresholds.put-pipeline.value': '200',\n",
       "  'cluster_manager.throttling.thresholds.put-repository.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.put-script.value': '200',\n",
       "  'cluster_manager.throttling.thresholds.remove-component-template.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.remove-data-stream.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.remove-index-template-v2.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.remove-index-template.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.remove-persistent-task.value': '200',\n",
       "  'cluster_manager.throttling.thresholds.restore-snapshot.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.rollover-index.value': '200',\n",
       "  'cluster_manager.throttling.thresholds.update-settings.value': '50',\n",
       "  'cluster_manager.throttling.thresholds.update-snapshot-state.value': '5000',\n",
       "  'cluster_manager.throttling.thresholds.update-task-state.value': '200',\n",
       "  'plugins.index_state_management.template_migration.control': '-1'},\n",
       " 'transient': {'cluster.routing.allocation.awareness.force.zone.values': 'xx-xxxxx-xx',\n",
       "  'cluster.routing.allocation.disk.watermark.flood_stage': '0.9758094787597656gb',\n",
       "  'cluster.routing.allocation.disk.watermark.high': '1.9516189575195313gb',\n",
       "  'cluster.routing.allocation.disk.watermark.low': '2.9274284362792966gb',\n",
       "  'cluster.routing.allocation.load_awareness.provisioned_capacity': '1',\n",
       "  'cluster.routing.allocation.load_awareness.skew_factor': '50.0',\n",
       "  'plugins.ml_commons.only_run_on_ml_node': 'false'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aos_client.cluster.get_settings(flat_settings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0742cd",
   "metadata": {},
   "source": [
    "### 8. Download pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bab973e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip',\n",
       " <http.client.HTTPMessage at 0x7efe5e9b7f40>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://github.com/opensearch-project/ml-commons/raw/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip?raw=true', 'model/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eacd3c",
   "metadata": {},
   "source": [
    "Verify model is downloaded successfully in the `model` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24144556",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 81468\r\n",
      "drwxrwxr-x 2 ec2-user ec2-user     4096 Apr 29 19:22 .\r\n",
      "drwxrwxr-x 9 ec2-user ec2-user     4096 Apr 29 19:20 ..\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user      279 Apr 29 17:16 all-MiniLM-L6-v2_torchscript.json\r\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 83408741 Apr 29 19:22 all-MiniLM-L6-v2_torchscript_sentence-transformer.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62421acd",
   "metadata": {},
   "source": [
    "### 9. Upload BERT model to OpenSearch domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cfc28ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opensearch_py_ml.ml_models import SentenceTransformerModel\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "\n",
    "ml_client = MLCommonClient(aos_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e606e695",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks 9\n",
      "Sha1 value of the model file:  9376c2ebd7c83f99ec2526323786c348d2382e6d86576f750c89ea544d6bbb14\n",
      "Model meta data was created successfully. Model Id:  Nq53zocBnCSv9UI7wh4r\n",
      "uploading chunk 1 of 9\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 2 of 9\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 3 of 9\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 4 of 9\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 5 of 9\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 6 of 9\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 7 of 9\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 8 of 9\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 9 of 9\n",
      "Model id: {'status': 'Uploaded'}\n",
      "Model uploaded successfully\n",
      "model id:Nq53zocBnCSv9UI7wh4r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = './model/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip'\n",
    "model_config_path = './model/all-MiniLM-L6-v2_torchscript.json'\n",
    "\n",
    "\n",
    "model_id=ml_client.upload_model(model_path, model_config_path, isVerbose=True)\n",
    "\n",
    "print(\"model id:\" + model_id)\n",
    "\n",
    "ml_client.unload_model(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f19624c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_id = model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9d104",
   "metadata": {},
   "source": [
    "### 10. Load the model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52cba357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_id': 'N653zocBnCSv9UI73R48', 'status': 'CREATED'}\n"
     ]
    }
   ],
   "source": [
    "load_model_output = ml_client.load_model(model_id)\n",
    "\n",
    "print(load_model_output)\n",
    "task_id = load_model_output['task_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f69aea",
   "metadata": {},
   "source": [
    "Get the task detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "510eea27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_id': 'Nq53zocBnCSv9UI7wh4r', 'task_type': 'LOAD_MODEL', 'function_name': 'TEXT_EMBEDDING', 'state': 'RUNNING', 'worker_node': ['HNnmWWEGQEm7JmYmjlIPFg'], 'create_time': 1682796174428, 'last_update_time': 1682796174705, 'is_async': True}\n"
     ]
    }
   ],
   "source": [
    "task_info = ml_client.get_task_info(task_id)\n",
    "\n",
    "print(task_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54be9c",
   "metadata": {},
   "source": [
    "Get the model detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1211a76d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'all-MiniLM-L6-v2', 'algorithm': 'TEXT_EMBEDDING', 'model_version': '1.1.0', 'model_format': 'TORCH_SCRIPT', 'model_state': 'LOADING', 'model_content_hash_value': '9376c2ebd7c83f99ec2526323786c348d2382e6d86576f750c89ea544d6bbb14', 'model_config': {'model_type': 'bert', 'embedding_dimension': 384, 'framework_type': 'SENTENCE_TRANSFORMERS', 'pooling_mode': 'MEAN', 'normalize_result': False}, 'created_time': 1682796167721, 'last_updated_time': 1682796174673, 'total_chunks': 9, 'planning_worker_node_count': 1, 'planning_worker_nodes': ['HNnmWWEGQEm7JmYmjlIPFg']}\n"
     ]
    }
   ],
   "source": [
    "model_info = ml_client.get_model_info(model_id)\n",
    "\n",
    "print(model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625b5cf",
   "metadata": {},
   "source": [
    "### 11. Create pipeline to convert text into vector with BERT model\n",
    "We will use the just uploaded model to convert `qestion` field into vector(embedding) and stored into `question_vector` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc810643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline={\n",
    "  \"description\": \"An example neural search pipeline\",\n",
    "  \"processors\" : [\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": model_id,\n",
    "        \"field_map\": {\n",
    "           \"description\": \"description_vector\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "pipeline_id = 'nlp_pipeline'\n",
    "aos_client.ingest.put_pipeline(id=pipeline_id,body=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431a804",
   "metadata": {},
   "source": [
    "Verify pipeline is created succefuflly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13ff2f91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nlp_pipeline': {'description': 'An example neural search pipeline',\n",
       "  'processors': [{'text_embedding': {'model_id': 'Nq53zocBnCSv9UI7wh4r',\n",
       "     'field_map': {'description': 'description_vector'}}}]}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aos_client.ingest.get_pipeline(id=pipeline_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaabc1e",
   "metadata": {},
   "source": [
    "### 12. Create a index in Amazon Opensearch Service \n",
    "Whereas we previously created an index with 2 fields, this time we'll define the index with 3 fields: the first field ' question_vector' holds the vector representation of the question, the second is the \"question\" for raw sentence and the third field is \"answer\" for the raw answer data.\n",
    "\n",
    "To create the index, we first define the index in JSON, then use the aos_client connection we initiated ealier to create the index in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eba5754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"default_pipeline\": pipeline_id,\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 384,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"l2\",\n",
    "                    \"engine\": \"faiss\"\n",
    "                },\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"designation\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"variety\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"country\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"winery\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"points\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330502a",
   "metadata": {},
   "source": [
    "If for any reason you need to recreate your dataset, you can uncomment and execute the following to delete any previously created indexes. If this is the first time you're running this, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a835b9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "NotFoundError(404, 'index_not_found_exception', 'no such index [nlp_wmd]', nlp_wmd, index_or_alias)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16263/3897168486.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maos_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nlp_wmd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# If this is the first time you're running this, you won't have this index to drop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/client/indices.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, index, params, headers)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty value passed for a required argument 'index'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         return self.transport.perform_request(\n\u001b[0m\u001b[1;32m    305\u001b[0m             \u001b[0;34m\"DELETE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_make_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                 status, headers_response, data = connection.perform_request(\n\u001b[0m\u001b[1;32m    371\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/connection/http_requests.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             )\n\u001b[0;32m--> 219\u001b[0;31m             self._raise_error(\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/connection/base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[0;34m(self, status_code, raw_data, content_type)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Undecodable raw error response from server: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         raise HTTP_EXCEPTIONS.get(status_code, TransportError)(\n\u001b[0m\u001b[1;32m    302\u001b[0m             \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: NotFoundError(404, 'index_not_found_exception', 'no such index [nlp_wmd]', nlp_wmd, index_or_alias)"
     ]
    }
   ],
   "source": [
    "# aos_client.indices.delete(index=\"nlp_pqa\") # drop the index from the previous lab\n",
    "\n",
    "\n",
    "aos_client.indices.delete(index=\"nlp_wmd\")\n",
    "# If this is the first time you're running this, you won't have this index to drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de634d",
   "metadata": {},
   "source": [
    "Using the above index definition, we now need to create the index in Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "715b751d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nlp_wmd'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aos_client.indices.create(index=\"nlp_wmd\",body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7007735",
   "metadata": {},
   "source": [
    "Let's verify the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f71659d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nlp_wmd': {'aliases': {},\n",
       "  'mappings': {'properties': {'country': {'type': 'text', 'store': True},\n",
       "    'description': {'type': 'text', 'store': True},\n",
       "    'description_vector': {'type': 'knn_vector',\n",
       "     'store': True,\n",
       "     'dimension': 384,\n",
       "     'method': {'engine': 'faiss',\n",
       "      'space_type': 'l2',\n",
       "      'name': 'hnsw',\n",
       "      'parameters': {}}},\n",
       "    'designation': {'type': 'text', 'store': True},\n",
       "    'points': {'type': 'integer', 'store': True},\n",
       "    'variety': {'type': 'text', 'store': True},\n",
       "    'winery': {'type': 'text', 'store': True}}},\n",
       "  'settings': {'index': {'number_of_shards': '5',\n",
       "    'provided_name': 'nlp_wmd',\n",
       "    'knn.space_type': 'cosinesimil',\n",
       "    'default_pipeline': 'nlp_pipeline',\n",
       "    'knn': 'true',\n",
       "    'creation_date': '1682796183404',\n",
       "    'analysis': {'analyzer': {'default': {'type': 'standard',\n",
       "       'stopwords': '_english_'}}},\n",
       "    'number_of_replicas': '1',\n",
       "    'uuid': '0UI3t7K1TkyRB9Ea-HZSTg',\n",
       "    'version': {'created': '136267827'}}}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aos_client.indices.get(index=\"nlp_wmd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040992c",
   "metadata": {},
   "source": [
    "### 13. Load the raw data into the Index\n",
    "Next, let's load the headset enhanced PQA data into the index we've just created. During ingest data, `question` field will also be converted to vector(embedding) by the `nlp_pipeline` we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e55e6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for c in wm_list:\n",
    "    content=c['description']\n",
    "    description=c['description']\n",
    "    points=c[\"points\"]\n",
    "    variety=c[\"variety\"]\n",
    "    country=c[\"country\"]\n",
    "    designation=c[\"designation\"]\n",
    "    winery=c[\"winery\"]\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "    aos_client.index(index='nlp_wmd',body={\n",
    "        \"content\": content,\n",
    "        \"points\": points,\n",
    "        \"variety\": variety,\n",
    "        \"country\": country,\n",
    "        \"description\": description,\n",
    "        \"designation\": designation,\n",
    "        \"winery\": winery,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fad674",
   "metadata": {},
   "source": [
    "To validate the load, we'll query the number of documents number in the index. We should have 300 hits in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05ed0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records found: 296.\n"
     ]
    }
   ],
   "source": [
    "res = aos_client.search(index=\"nlp_wmd\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b827c",
   "metadata": {},
   "source": [
    "### 14. Search vector with \"Semantic Search\" \n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a wine whose review most closely matches the requested description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8ed4ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 290 Hits:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': \"Dark and thick, this full-bodied wine from the Sierra region brings truckloads of jammy fruit flavors along with a firm texture that doesn't overly tighten the tannic grip. The blackberry and black-cherry notes are simply delicious.\",\n",
       " 'winery': 'Wilderotter',\n",
       " 'points': 90,\n",
       " 'designation': None,\n",
       " 'country': 'US',\n",
       " 'variety': 'Petite Sirah'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_wines(desired_description, n=1):\n",
    "    osquery={\n",
    "      \"_source\": {\n",
    "            \"exclude\": [ \"description_vector\" ]\n",
    "        },\n",
    "      \"size\": 30,\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"description_vector\": {\n",
    "            \"query_text\": desired_description,\n",
    "            \"model_id\": bert_model_id,\n",
    "            \"k\": 30\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=\"nlp_wmd\", \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"description\",\"winery\",\"points\", \"designation\", \"country\"])\n",
    "\n",
    "    print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "    query_result=[]\n",
    "    for hit in res['hits']['hits']:\n",
    "        row=[\n",
    "                hit['_id'],\n",
    "                hit['_score'],\n",
    "                hit['_source']['description'],\n",
    "                hit['_source']['winery'],\n",
    "                hit['_source']['points'],\n",
    "                hit['_source']['designation'],\n",
    "                hit['_source']['country'],\n",
    "                hit['_source']['variety'],\n",
    "            ]\n",
    "        query_result.append(row)\n",
    "\n",
    "    query_result_df = pd.DataFrame(data=query_result,columns=[\n",
    "                                                            \"_id\",\n",
    "                                                            \"_score\",\n",
    "                                                            \"description\",\n",
    "                                                            \"winery\", \n",
    "                                                            \"points\", \n",
    "                                                            \"designation\",\n",
    "                                                            \"country\",     \n",
    "                                                            \"variety\",\n",
    "                                                         ])\n",
    "    \n",
    "    query_result_df.drop(['_id', '_score'], inplace=True, axis=1)\n",
    "    result = query_result_df.head(n).to_dict('records')\n",
    "    return result\n",
    "\n",
    "\n",
    "example_request = 'big and bold, jammy, blackberries'\n",
    "example_review = query_wines(example_request, 2)[0]\n",
    "\n",
    "example_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abddaa4",
   "metadata": {},
   "source": [
    "### 15. Deploy the LLM\n",
    "\n",
    "This lab uses [AlexaTM 20B](https://aws.amazon.com/about-aws/whats-new/2022/11/alexatm-20b-model-available-sagemaker-jumpstart/) model to create recomendations based on a given wine review. The next cell deploys a model endpoint into your environment that will be called by subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c652c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☕ Spinning up the endpoint. This will take a little while ☕\n",
      "------------------!"
     ]
    }
   ],
   "source": [
    "import sagemaker, json\n",
    "from sagemaker import get_execution_role\n",
    "from datetime import datetime\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "\n",
    "# model_version = \"*\" fetches the latest version of the model\n",
    "model_id, model_version = \"pytorch-textgeneration1-alexa20b\", \"*\"\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-console-infer-{model_id}\")\n",
    "\n",
    "endpoint_config_name = \"config-\" + endpoint_name\n",
    "\n",
    "\n",
    "# GPU Instance Reqts: >50 GB of CPU RAM and >42 GB of GPU memory in total\n",
    "# Tested with ml.g4dn.12xlarge, ml.p3.8xlarge and ml.p3.16xlarge\n",
    "# instance_type = \"ml.g4dn.12xlarge\"\n",
    "instance_type = \"ml.p3.8xlarge\"\n",
    "\n",
    "# If using an EBS-backed instance, you must specify at least 256 GB of storage\n",
    "# If using an instance with local SSD storage, volume_size must be None\n",
    "if instance_type == \"ml.g4dn.12xlarge\":\n",
    "    volume_size = None\n",
    "elif instance_type in [\"ml.p3.8xlarge\", \"ml.p3.16xlarge\"]:\n",
    "    volume_size = 256\n",
    "else:\n",
    "    volume_size = None\n",
    "    print(\n",
    "        f\"Instance_type={instance_type} not tested. Setting volume_size = None.\"\n",
    "        \"If you run into out of space errors and your instance supports EBS storage,\"\n",
    "        \"please set volume_size = 256.\"\n",
    "    )\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base PyTorch container image.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri. This includes both pre-trained parameters, inference handling scripts and any dependencies.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "env = {\n",
    "    \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": str(7200),\n",
    "    \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
    "    \"SAGEMAKER_ENV\": \"1\",\n",
    "    \"SAGEMAKER_SUBMIT_DIRECTORY\": \"/opt/ml/model/code/\",\n",
    "    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
    "    \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",  # without this, there will be one process per GPU\n",
    "    \"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\",  # without this, each worker will have 1/num_gpus the RAM\n",
    "}\n",
    "\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    "    env=env,\n",
    ")\n",
    "\n",
    "print(\"☕ Spinning up the endpoint. This will take a little while ☕\")\n",
    "\n",
    "# deploy the Model.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    volume_size=volume_size,  # Specify the size of the Amazon EBS volume.\n",
    "    model_data_download_timeout=3600,  # Specify the model download timeout in seconds.\n",
    "    container_startup_health_check_timeout=3600,  # Specify the health checkup timeout in seconds\n",
    ")\n",
    "\n",
    "# If you already deployed a model, comment the above model_predictory instantiation using model.deploy() and\n",
    "# add your endpoint name below\n",
    "# model_predictor = Predictor(endpoint_name=\"jumpstart-console-infer-pytorch-textgen-2023-04-11-01-23-42-572\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e1cf6",
   "metadata": {},
   "source": [
    "### 15 Query the LLM with a test recomendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb0d6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(model_predictor, text, generate_kwargs=None, max_num_attempts=5):\n",
    "    \"\"\"Query the model predictor.\n",
    "\n",
    "    model_predictor: The deployed model pipeline.\n",
    "    text: a string or list of strings to input to the model pipeline.\n",
    "    generate_kwargs: A dictionary of generation arguments.\n",
    "    max_num_attempts: Maximum number of invokation request.\n",
    "\n",
    "    returns: A JSON of the model outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    payload = {\"text_inputs\": text}\n",
    "    if generate_kwargs is not None:\n",
    "        payload.update(generate_kwargs)\n",
    "\n",
    "    encoded_inp = json.dumps(payload).encode(\"utf-8\")\n",
    "    for _ in range(max_num_attempts):\n",
    "        try:\n",
    "            query_response = model_predictor.predict(\n",
    "                encoded_inp,\n",
    "                {\"ContentType\": \"application/json\", \"Accept\": \"application/json\"},\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Invokation request unsuccessful. Retrying.\")\n",
    "            continue\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    \"\"\"Parse response and return the list of generated texts.\"\"\"\n",
    "\n",
    "    return json.loads(query_response)[\"generated_texts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48a35d2",
   "metadata": {},
   "source": [
    "### 16. Create a prompt using the search results\n",
    "\n",
    "The AlexaTM 20B model performs well for single shot generation. The render_prompt function creates a single shot prompt using the requested description and a pre-made example response. You can read more about the AlexaTM 20B model on [Amazon Science](https://www.amazon.science/publications/alexatm-20b-few-shot-learning-using-a-large-scale-multilingual-seq2seq-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5783ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 290 Hits:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLM] Context: A sommelier uses their vast knowledge of wine to make great recomendations people will enjoy. A recomendation always includes the winery, the country of origin, and a colorful description.Data: <br> Recomendation: <br>Data: {{\\'description\\': \\'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.\\', \\'winery\\': \\'Darioush\\', \\'points\\': 92, \\'designation\\': None, \\'country\\': \\'US\\'}} <br> Recomendation: I have a wonderful wine for you. It\\'s a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator. <br>Data: {\\'description\\': \"Dark and thick, this full-bodied wine from the Sierra region brings truckloads of jammy fruit flavors along with a firm texture that doesn\\'t overly tighten the tannic grip. The blackberry and black-cherry notes are simply delicious.\", \\'winery\\': \\'Wilderotter\\', \\'points\\': 90, \\'designation\\': None, \\'country\\': \\'US\\', \\'variety\\': \\'Petite Sirah\\'} <br> Recomendation:'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def render_prompt(requested_description):\n",
    "    recomendation = query_wines(requested_description)[0]\n",
    "#     print(f\"Recomendation: {recomendation}\")\n",
    "    sample_recomendation = \"{{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}}\"\n",
    "    sample_response = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "    \n",
    "    prompt = (\n",
    "        f\"[CLM] Context: A sommelier uses their vast knowledge of wine to make great recomendations people will enjoy. A recomendation always includes the winery, the country of origin, and a colorful description.\"\n",
    "        f\"Data: <br> Recomendation: <br>\"\n",
    "        f\"Data: {sample_recomendation} <br> Recomendation: {sample_response} <br>\"\n",
    "        f\"Data: {recomendation} <br> Recomendation:\"\n",
    "    )\n",
    "    \n",
    "#     print(f\"Rendered Prompt: {prompt}\")\n",
    "    return prompt\n",
    "\n",
    "prompt = render_prompt(example_request) # query with the same sample description from earlier\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e0058",
   "metadata": {},
   "source": [
    "### 17. Query the LLM using the rendered prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c263ba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I have a wonderful wine for you. It's a dry, medium bodied red wine from Wilderotter winery in the Sierra region of California, US. It has flavors of blackberry and black-cherry. It scored 90 points in wine spectator. <br>Data: {'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': {{'name':'Darioush','points': 92,'designation': None,'country': ''}}'}}} <br\"]\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"num_beams\": 5, \n",
    "    \"no_repeat_ngram_size\": 3, \n",
    "    \"temperature\": 1.25, \n",
    "#     \"top_p\": .8,\n",
    "    \"top_k\": 147,\n",
    "    \"max_length\": 175,\n",
    "    \"early_stopping\": True,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "query_response = query(model_predictor, prompt, kwargs)\n",
    "generated_texts = parse_response(query_response)\n",
    "\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ede14",
   "metadata": {},
   "source": [
    "### 18. Writing the wine recomender function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0015505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 290 Hits:\n"
     ]
    }
   ],
   "source": [
    "def recomend_wine(requested_description,\n",
    "                 num_beams=7,\n",
    "                 no_repeat_ngram_size=3,\n",
    "                 temperature=1.0,\n",
    "                 top_k=147,\n",
    "                 max_length=250,\n",
    "                 early_stopping=True,\n",
    "                 num_return_sequences=1,\n",
    "                 seed=0):\n",
    "    \n",
    "    prompt = render_prompt(requested_description) # query the vector store and render the prompt\n",
    "    query_response = query(model_predictor, prompt, kwargs) # query the llm with the prompt\n",
    "    generated_texts = parse_response(query_response) \n",
    "    \n",
    "    return generated_texts[0].split('<br>')[0]\n",
    "\n",
    "recommendation = recomend_wine('dessert wine. pairs with chocolate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8165166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a wonderful wine for you. It's a dry, medium bodied red wine from Eradus winery in the Marlborough region of New Zealand. It has flavors of raspberry, chocolate, black olive, and green herbs. It scored 92 points in wine spectator. \n"
     ]
    }
   ],
   "source": [
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8822ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
