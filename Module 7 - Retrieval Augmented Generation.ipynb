{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# Semantic Search with OpenSearch Neural Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "We will use the semantic search to provide the best matching wine based on the review description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31703e3d",
   "metadata": {},
   "source": [
    "### 1. Check PyTorch Version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac12126",
   "metadata": {},
   "source": [
    "As in the previous modules, let's import PyTorch and confirm that have have the latest version of PyTorch. The version should already be 1.10.2 or higher. If not, please run the lab in order to get everything set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b532987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f1cc51",
   "metadata": {},
   "source": [
    "### 2. Retrieve notebook variables\n",
    "\n",
    "The line below will retrieve your shared variables from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0e06e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3fa4b0",
   "metadata": {},
   "source": [
    "### 3. Install OpenSearch ML Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1c491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opensearch-py-ml\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c00375",
   "metadata": {},
   "source": [
    "Now we need to restart the kernel by running below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94df946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa614bc",
   "metadata": {},
   "source": [
    "### 4. Import library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6607721",
   "metadata": {},
   "source": [
    "### 5. Prepare Headset WMD data\n",
    "You can download the dataset from various sources. One is Kaggle.\n",
    "https://www.kaggle.com/datasets/christopheiv/winemagdata130k?select=winemag-data-130k-v2.json\n",
    "\n",
    "After downloading and copying here, unzip in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fca957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/christopheiv/winemagdata130k?select=winemag-data-130k-v2.json\n",
    "\n",
    "!unzip -o winemag-data-130k-v2.json.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a462a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('winemag-data-130k-v2.json')\n",
    "\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1cf47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# wm_list = df.to_dict('records')\n",
    "wm_list = df.sample(500).to_dict('records') # sample to keep lab quick\n",
    "\n",
    "wm_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b402353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f54a349",
   "metadata": {},
   "source": [
    "### 6. Create an OpenSearch cluster connection.\n",
    "Next, we'll use Python API to set up connection with OpenSearch Cluster.\n",
    "\n",
    "Note: if you're using a region other than us-east-1, please update the region in the code below.\n",
    "\n",
    "#### Get Cloud Formation stack output variables\n",
    "\n",
    "We also need to grab some key values from the infrastructure we provisioned using CloudFormation. To do this, we will list the outputs from the stack and store this in \"outputs\" to be used later.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "bucket = outputs['s3BucketTraining']\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "\n",
    "region = 'us-east-1' \n",
    "\n",
    "#credentials = boto3.Session().get_credentials()\n",
    "#auth = AWSV4SignerAuth(credentials, region)\n",
    "auth = (\"master\",\"Semantic123!\")\n",
    "index_name = 'nlp_wmd'\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173ff23",
   "metadata": {},
   "source": [
    "### 7. Configure OpenSearch domain to enable run Machine Learning code in data node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4080d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = b'{\"transient\":{\"plugins.ml_commons.only_run_on_ml_node\": false}}'\n",
    "aos_client.cluster.put_settings(body=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a9cd9",
   "metadata": {},
   "source": [
    "Verify `plugins.ml_commons.only_run_on_ml_node` is set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6a646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.cluster.get_settings(flat_settings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0742cd",
   "metadata": {},
   "source": [
    "### 8. Download pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab973e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://github.com/opensearch-project/ml-commons/raw/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip?raw=true', 'model/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eacd3c",
   "metadata": {},
   "source": [
    "Verify model is downloaded successfully in the `model` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24144556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -al model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62421acd",
   "metadata": {},
   "source": [
    "### 9. Upload BERT model to OpenSearch domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc28ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opensearch_py_ml.ml_models import SentenceTransformerModel\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "\n",
    "ml_client = MLCommonClient(aos_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e606e695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model_path = './model/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip'\n",
    "model_config_path = './model/all-MiniLM-L6-v2_torchscript.json'\n",
    "\n",
    "\n",
    "model_id=ml_client.upload_model(model_path, model_config_path, isVerbose=True)\n",
    "\n",
    "print(\"model id:\" + model_id)\n",
    "\n",
    "ml_client.unload_model(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9d104",
   "metadata": {},
   "source": [
    "### 10. Load the model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cba357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_model_output = ml_client.load_model(model_id)\n",
    "\n",
    "print(load_model_output)\n",
    "task_id = load_model_output['task_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f69aea",
   "metadata": {},
   "source": [
    "Get the task detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510eea27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_info = ml_client.get_task_info(task_id)\n",
    "\n",
    "print(task_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54be9c",
   "metadata": {},
   "source": [
    "Get the model detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211a76d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_info = ml_client.get_model_info(model_id)\n",
    "\n",
    "print(model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625b5cf",
   "metadata": {},
   "source": [
    "### 11. Create pipeline to convert text into vector with BERT model\n",
    "We will use the just uploaded model to convert `qestion` field into vector(embedding) and stored into `question_vector` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc810643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline={\n",
    "  \"description\": \"An example neural search pipeline\",\n",
    "  \"processors\" : [\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": model_id,\n",
    "        \"field_map\": {\n",
    "           \"description\": \"description_vector\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "pipeline_id = 'nlp_pipeline'\n",
    "aos_client.ingest.put_pipeline(id=pipeline_id,body=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431a804",
   "metadata": {},
   "source": [
    "Verify pipeline is created succefuflly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff2f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.ingest.get_pipeline(id=pipeline_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaabc1e",
   "metadata": {},
   "source": [
    "### 12. Create a index in Amazon Opensearch Service \n",
    "Whereas we previously created an index with 2 fields, this time we'll define the index with 3 fields: the first field ' question_vector' holds the vector representation of the question, the second is the \"question\" for raw sentence and the third field is \"answer\" for the raw answer data.\n",
    "\n",
    "To create the index, we first define the index in JSON, then use the aos_client connection we initiated ealier to create the index in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"default_pipeline\": pipeline_id,\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 384,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"l2\",\n",
    "                    \"engine\": \"faiss\"\n",
    "                },\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"designation\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"variety\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"country\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"winery\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"points\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330502a",
   "metadata": {},
   "source": [
    "If for any reason you need to recreate your dataset, you can uncomment and execute the following to delete any previously created indexes. If this is the first time you're running this, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835b9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.delete(index=\"nlp_wmd\")\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de634d",
   "metadata": {},
   "source": [
    "Using the above index definition, we now need to create the index in Amazon OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b751d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.create(index=\"nlp_wmd\",body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7007735",
   "metadata": {},
   "source": [
    "Let's verify the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71659d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=\"nlp_wmd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040992c",
   "metadata": {},
   "source": [
    "### 13. Load the raw data into the Index\n",
    "Next, let's load the headset enhanced PQA data into the index we've just created. During ingest data, `question` field will also be converted to vector(embedding) by the `nlp_pipeline` we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55e6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for c in wm_list:\n",
    "    content=c['description']\n",
    "    description=c['description']\n",
    "    points=c[\"points\"]\n",
    "    variety=c[\"variety\"]\n",
    "    country=c[\"country\"]\n",
    "    designation=c[\"designation\"]\n",
    "    winery=c[\"winery\"]\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "    aos_client.index(index='nlp_wmd',body={\n",
    "        \"content\": content,\n",
    "        \"points\": points,\n",
    "        \"variety\": variety,\n",
    "        \"country\": country,\n",
    "        \"description\": description,\n",
    "        \"designation\": designation,\n",
    "        \"winery\": winery,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fad674",
   "metadata": {},
   "source": [
    "To validate the load, we'll query the number of documents number in the index. We should have 1000 hits in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = aos_client.search(index=\"nlp_wmd\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f9063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b827c",
   "metadata": {},
   "source": [
    "### 14. Search vector with \"Semantic Search\" \n",
    "\n",
    "We can search the data with neural search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5f4e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query={\n",
    "  \"_source\": {\n",
    "        \"exclude\": [ \"description_vector\" ]\n",
    "    },\n",
    "  \"size\": 30,\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"description_vector\": {\n",
    "        \"query_text\": \"big bold cab with berries and cherries\",\n",
    "        \"model_id\": model_id,\n",
    "        \"k\": 30\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "res = aos_client.search(index=\"nlp_wmd\", \n",
    "                       body=query,\n",
    "                       stored_fields=[\"description\",\"winery\",\"points\", \"designation\", \"country\"])\n",
    "\n",
    "print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "query_result=[]\n",
    "for hit in res['hits']['hits']:\n",
    "    row=[\n",
    "            hit['_id'],\n",
    "            hit['_score'],\n",
    "            hit['_source']['description'],\n",
    "            hit['_source']['winery'],\n",
    "            hit['_source']['points'],\n",
    "            hit['_source']['designation'],\n",
    "            hit['_source']['country'],\n",
    "        ]\n",
    "    query_result.append(row)\n",
    "    \n",
    "query_result[0]\n",
    "\n",
    "query_result_df = pd.DataFrame(data=query_result,columns=[\n",
    "                                                        \"_id\",\n",
    "                                                        \"_score\",\n",
    "                                                        \"description\",\n",
    "                                                        \"winery\", \n",
    "                                                        \"points\", \n",
    "                                                        \"designation\",\n",
    "                                                        \"country\",                                                                        \n",
    "                                                     ])\n",
    "display(query_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abddaa4",
   "metadata": {},
   "source": [
    "import sagemaker, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c652c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, json\n",
    "from sagemaker import get_execution_role\n",
    "from datetime import datetime\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "\n",
    "# model_version = \"*\" fetches the latest version of the model\n",
    "model_id, model_version = \"pytorch-textgeneration1-alexa20b\", \"*\"\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-console-infer-{model_id}\")\n",
    "\n",
    "endpoint_config_name = \"config-\" + endpoint_name\n",
    "\n",
    "\n",
    "# GPU Instance Reqts: >50 GB of CPU RAM and >42 GB of GPU memory in total\n",
    "# Tested with ml.g4dn.12xlarge, ml.p3.8xlarge and ml.p3.16xlarge\n",
    "instance_type = \"ml.g4dn.12xlarge\"\n",
    "\n",
    "# If using an EBS-backed instance, you must specify at least 256 GB of storage\n",
    "# If using an instance with local SSD storage, volume_size must be None\n",
    "if instance_type == \"ml.g4dn.12xlarge\":\n",
    "    volume_size = None\n",
    "elif instance_type in [\"ml.p3.8xlarge\", \"ml.p3.16xlarge\"]:\n",
    "    volume_size = 256\n",
    "else:\n",
    "    volume_size = None\n",
    "    print(\n",
    "        f\"Instance_type={instance_type} not tested. Setting volume_size = None.\"\n",
    "        \"If you run into out of space errors and your instance supports EBS storage,\"\n",
    "        \"please set volume_size = 256.\"\n",
    "    )\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base PyTorch container image.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri. This includes both pre-trained parameters, inference handling scripts and any dependencies.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "env = {\n",
    "    \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": str(3600),\n",
    "    \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
    "    \"SAGEMAKER_ENV\": \"1\",\n",
    "    \"SAGEMAKER_SUBMIT_DIRECTORY\": \"/opt/ml/model/code/\",\n",
    "    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
    "    \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",  # without this, there will be one process per GPU\n",
    "    \"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\",  # without this, each worker will have 1/num_gpus the RAM\n",
    "}\n",
    "\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    "    env=env,\n",
    ")\n",
    "\n",
    "print(\"☕ Spinning up the endpoint. This will take a little while ☕\")\n",
    "\n",
    "# deploy the Model.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    volume_size=volume_size,  # Specify the size of the Amazon EBS volume.\n",
    "    model_data_download_timeout=3600,  # Specify the model download timeout in seconds.\n",
    "    container_startup_health_check_timeout=3600,  # Specify the health checkup timeout in seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(model_predictor, text, generate_kwargs=None, max_num_attempts=5):\n",
    "    \"\"\"Query the model predictor.\n",
    "\n",
    "    model_predictor: The deployed model pipeline.\n",
    "    text: a string or list of strings to input to the model pipeline.\n",
    "    generate_kwargs: A dictionary of generation arguments.\n",
    "    max_num_attempts: Maximum number of invokation request.\n",
    "\n",
    "    returns: A JSON of the model outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    payload = {\"text_inputs\": text}\n",
    "    if generate_kwargs is not None:\n",
    "        payload.update(generate_kwargs)\n",
    "\n",
    "    encoded_inp = json.dumps(payload).encode(\"utf-8\")\n",
    "    for _ in range(max_num_attempts):\n",
    "        try:\n",
    "            query_response = model_predictor.predict(\n",
    "                encoded_inp,\n",
    "                {\"ContentType\": \"application/json\", \"Accept\": \"application/json\"},\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Invokation request unsuccessful. Retrying.\")\n",
    "            continue\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    \"\"\"Parse response and return the list of generated texts.\"\"\"\n",
    "\n",
    "    return json.loads(query_response)[\"generated_texts\"]\n",
    "\n",
    "\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "text = f\"[CLM] Wine Recomendation: [{{'description': 'Big, tough, gutsy, fruity, tannic. In other words, Petite Sirah, and classic at that. Shows very ripe, deep and long-lasting flavors of blackberries, blueberries, currants, chocolate, cedar and spices, in a bone-dry, full-bodied red wine. Good now, and should develop over a decade.','winery': 'Field Stone','points': 92,'designation': 'Staten Family Reserve','country': 'US'}}] ==> My Recomendation: [You should try Staten Family Reserve by Field Stone in the US. I was blown away the first time I tried it. It's an instant classic. Big, tough, gutsy, fruity, tannic. It's a dry, full bodied wine flavors of blackberries, blueberries, currants, chocolate, cedar and spices and scored 92 points in wine spectator.] <br><br><br> Wine Recomendation [{recomendation}] ==> My Recomendation:\"\n",
    "\n",
    "kwargs = {\n",
    "    \"num_beams\": 5, \n",
    "    \"no_repeat_ngram_size\": 3, \n",
    "    \"temperature\": 1, \n",
    "#     \"top_p\": .8,\n",
    "    \"top_k\": 147,\n",
    "    \"max_length\": 250,\n",
    "    \"early_stopping\": True,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "query_response = query(model_predictor, text, kwargs)\n",
    "generated_texts = parse_response(query_response)\n",
    "print(f\"Input text: {text}{newline}\" f\"Generated text: {bold}{generated_texts}{unbold}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c263ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_wines(desired_description, n=1):\n",
    "    osquery={\n",
    "      \"_source\": {\n",
    "            \"exclude\": [ \"description_vector\" ]\n",
    "        },\n",
    "      \"size\": 30,\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"description_vector\": {\n",
    "            \"query_text\": desired_description,\n",
    "            \"model_id\": model_id,\n",
    "            \"k\": 30\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=\"nlp_wmd\", \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"description\",\"winery\",\"points\", \"designation\", \"country\"])\n",
    "\n",
    "    print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "    query_result=[]\n",
    "    for hit in res['hits']['hits']:\n",
    "        row=[\n",
    "                hit['_id'],\n",
    "                hit['_score'],\n",
    "                hit['_source']['description'],\n",
    "                hit['_source']['winery'],\n",
    "                hit['_source']['points'],\n",
    "                hit['_source']['designation'],\n",
    "                hit['_source']['country'],\n",
    "            ]\n",
    "        query_result.append(row)\n",
    "\n",
    "    query_result_df = pd.DataFrame(data=query_result,columns=[\n",
    "                                                            \"_id\",\n",
    "                                                            \"_score\",\n",
    "                                                            \"description\",\n",
    "                                                            \"winery\", \n",
    "                                                            \"points\", \n",
    "                                                            \"designation\",\n",
    "                                                            \"country\",                                                                        \n",
    "                                                         ])\n",
    "    \n",
    "    query_result_df.drop(['_id', '_score'], inplace=True, axis=1)\n",
    "    result = query_result_df.head(n).to_dict('records')\n",
    "    return result\n",
    "\n",
    "query_wines('big and bold, jammy, blackberries', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35944c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ae98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bcea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prompt(requested_description):\n",
    "    recomendation = query_wines(requested_description)[0]\n",
    "    prompt = f\"[CLM] Wine Recomendation: [{{'description': 'Big, tough, gutsy, fruity, tannic. In other words, Petite Sirah, and classic at that. Shows very ripe, deep and long-lasting flavors of blackberries, blueberries, currants, chocolate, cedar and spices, in a bone-dry, full-bodied red wine. Good now, and should develop over a decade.','winery': 'Field Stone','points': 92,'designation': 'Staten Family Reserve','country': 'US'}}] ==> My Recomendation: [You should try Staten Family Reserve by Field Stone in the US. I was blown away the first time I tried it. It's an instant classic. Big, tough, gutsy, fruity, tannic. It's a dry, full bodied wine flavors of blackberries, blueberries, currants, chocolate, cedar and spices and scored 92 points in wine spectator.] <br><br><br> Wine Recomendation [{recomendation}] ==> My Recomendation:\"\n",
    "    return prompt\n",
    "\n",
    "prompt = render_prompt(\"light, fruity goes great with fish\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = query(model_predictor, prompt, kwargs)\n",
    "generated_texts = parse_response(query_response)\n",
    "\n",
    "print(f\"Input text: {prompt}{newline}\" f\"Generated text: {bold}{generated_texts}{unbold}{newline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb777d3d",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d80ff",
   "metadata": {},
   "source": [
    "### 17. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in prompt.split(\"\\n\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12b4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0015505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
