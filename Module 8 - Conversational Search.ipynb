{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d41ec6",
   "metadata": {},
   "source": [
    "# Conversational Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06184986",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets==8.1.0 --quiet\n",
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "from ipywidgets import Dropdown\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7486022",
   "metadata": {},
   "source": [
    "## Deploy embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id, embedding_model_version = (\n",
    "    \"huggingface-textembedding-gpt-j-6b-fp16\",\n",
    "    \"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef212514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "embedding_endpoint_name = name_from_base(f\"RAG-embedding-{embedding_model_id}\")\n",
    "\n",
    "embedding_instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "\n",
    "embedding_deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=embedding_model_id,\n",
    "    model_version=embedding_model_version,\n",
    "    instance_type=embedding_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "embedding_model_uri = model_uris.retrieve(\n",
    "    model_id=embedding_model_id, model_version=embedding_model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "embedding_model = Model(\n",
    "    image_uri=embedding_deploy_image_uri,\n",
    "    model_data=embedding_model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=embedding_endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "embedding_model_predictor = embedding_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=embedding_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2560f96",
   "metadata": {},
   "source": [
    "## Deploy content genration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_id, llm_model_version, = (\n",
    "    \"huggingface-llm-falcon-7b-instruct-bf16\",\n",
    "    \"*\",\n",
    ")\n",
    "\n",
    "llm_model_ids = ['huggingface-llm-falcon-40b-bf16',\n",
    "             'huggingface-llm-falcon-40b-instruct-bf16',\n",
    "             'huggingface-llm-falcon-7b-bf16',\n",
    "             'huggingface-llm-falcon-7b-instruct-bf16']\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = Dropdown(\n",
    "    options=llm_model_ids,\n",
    "    value=llm_model_id,\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c243cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_id = model_dropdown.value\n",
    "print(llm_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_endpoint_name = name_from_base(f\"RAG-LLM-{llm_model_id}\")\n",
    "\n",
    "llm_inference_instance_type = \"ml.g5.4xlarge\"\n",
    "\n",
    "health_check_timeout = 1800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff14a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "llm_model = JumpStartModel(model_id=llm_model_id, instance_type=llm_inference_instance_type)\n",
    "llm_model.env['SM_NUM_GPUS'] = '1'\n",
    "llm_model_predictor = llm_model.deploy(\n",
    "    endpoint_name=llm_endpoint_name,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f3fa2",
   "metadata": {},
   "source": [
    "## deploy with more code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_gpu = 4\n",
    "max_input_length = 1024\n",
    "max_total_tokens = 2048\n",
    "\n",
    "model_env = {\n",
    "    'HF_MODEL_ID': \"tiiuae/falcon-40b-instruct\",\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "    'MAX_INPUT_LENGTH': json.dumps(max_input_length),\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(max_total_tokens),\n",
    "}\n",
    "\n",
    "llm_endpoint_name = name_from_base(f\"RAG-LLM-{llm_model_id}\")\n",
    "\n",
    "llm_deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=llm_model_id,\n",
    "    model_version=llm_model_version,\n",
    "    instance_type=llm_inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "llm_model_uri = model_uris.retrieve(\n",
    "    model_id=llm_model_id, model_version=llm_model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "llm_model = Model(\n",
    "    image_uri=llm_deploy_image_uri,\n",
    "    model_data=llm_model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=llm_endpoint_name,\n",
    "    env=model_env\n",
    ")\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "llm_model_predictor = llm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=llm_inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=llm_endpoint_name,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff46271",
   "metadata": {},
   "source": [
    "## Get deployed endpoint for embedding and content generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49640d",
   "metadata": {},
   "source": [
    "### Get endpoint for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5962e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef415501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "\n",
    "describe_embedding_endpoint_response = sm_client.describe_endpoint(EndpointName=embedding_endpoint_name)\n",
    "\n",
    "while describe_embedding_endpoint_response[\"EndpointStatus\"] == 'Creating':\n",
    "    time.sleep(15)\n",
    "    print('.', end='')\n",
    "    describe_embedding_endpoint_response = sm_client.describe_endpoint(EndpointName=embedding_endpoint_name)\n",
    "print('enmbedding endpoint created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b48027",
   "metadata": {},
   "source": [
    "### Get endpoint for content generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecca035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm_endpoint_name='RAG-LLM-huggingface-llm-falcon-7b-instr-2023-08-20-12-32-31-953'\n",
    "print(llm_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d029d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "\n",
    "describe_llm_endpoint_response = sm_client.describe_endpoint(EndpointName=llm_endpoint_name)\n",
    "\n",
    "while describe_llm_endpoint_response[\"EndpointStatus\"] == 'Creating':\n",
    "    time.sleep(15)\n",
    "    print('.', end='')\n",
    "    describe_llm_endpoint_response = sm_client.describe_endpoint(EndpointName=llm_endpoint_name)\n",
    "print('LLM endpoint created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747507b",
   "metadata": {},
   "source": [
    "## Test embedding endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "\n",
    "\n",
    "class TestContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        print(response_json)\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        if len(embeddings) == 1:\n",
    "            return [embeddings[0]]\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "test_content_handler = TestContentHandler()\n",
    "\n",
    "test_embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=aws_region,\n",
    "    content_handler=test_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_embeddings.embed_documents([\"Hello World\"])[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa760ac7",
   "metadata": {},
   "source": [
    "## Test LLM endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86de9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "#method used to parse the inference model's response. we pass it as part of the model's config\n",
    "def parse_response_model(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return [gen[\"generated_text\"] for gen in model_predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which instances can I use with Managed Spot Training in Amazon SageMaker?\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": question,\n",
    "    \"parameters\":{\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": True,\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(\n",
    "    json.dumps(payload).encode(\"utf-8\"), endpoint_name=llm_endpoint_name\n",
    ")\n",
    "\n",
    "generated_texts = parse_response_model(query_response)\n",
    "\n",
    "print(f\"For model: {llm_endpoint_name}, \\n\\nThe generated output is: {generated_texts[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173a2d8",
   "metadata": {},
   "source": [
    "## Ingest documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70e437",
   "metadata": {},
   "source": [
    "### Install dependecy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97add515",
   "metadata": {},
   "source": [
    "### Define LangChain embedding with SageMaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f33062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import json\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "\n",
    "class BulkSagemakerEndpointEmbeddings(SagemakerEndpointEmbeddings):\n",
    "        def embed_documents(\n",
    "            self, texts: List[str], chunk_size: int = 5\n",
    "        ) -> List[List[float]]:\n",
    "            \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "            Args:\n",
    "                texts: The list of texts to embed.\n",
    "                chunk_size: The chunk size defines how many input texts will\n",
    "                    be grouped together as request. If None, will use the\n",
    "                    chunk size specified by the class.\n",
    "\n",
    "            Returns:\n",
    "                List of embeddings, one for each text.\n",
    "            \"\"\"\n",
    "            results = []\n",
    "            _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "            for i in range(0, len(texts), _chunk_size):\n",
    "                response = self._embedding_func(texts[i:i + _chunk_size])\n",
    "                results.extend(response)\n",
    "            return results\n",
    "        \n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "\n",
    "            input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "            return input_str.encode('utf-8') \n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            embeddings = response_json[\"embedding\"]\n",
    "            if len(embeddings) == 1:\n",
    "                return [embeddings[0]]\n",
    "            return embeddings\n",
    "        \n",
    "embeddings = BulkSagemakerEndpointEmbeddings( \n",
    "            endpoint_name=embedding_endpoint_name,\n",
    "            region_name=aws_region, \n",
    "            content_handler=ContentHandler())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e853a20",
   "metadata": {},
   "source": [
    "### Convert PDF content into vector and store into OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "#pdf_reader = PdfReader(\"opensearch-service-dg.pdf\")\n",
    "\n",
    "pdf_reader = PdfReader(\"c5-Fivetran.pdf\")\n",
    "\n",
    "text = \"\"\n",
    "for page in pdf_reader.pages:\n",
    "    text += page.extract_text()\n",
    "    \n",
    "\n",
    "    \n",
    "text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(len(chunks))\n",
    "\n",
    "DOMAIN_ADMIN_UNAME = 'admin'\n",
    "DOMAIN_ADMIN_PW = 'Awsadmin1!'\n",
    "DOMAIN_ENDPOINT = 'search-opensearchservi-wl9zlhduvblq-t44uhffaksglcvhreaxgtbjufe.us-east-1.es.amazonaws.com'\n",
    "os_domain_ep = 'https://'+DOMAIN_ENDPOINT\n",
    "\n",
    "embedding_index_name = 'embed_test_vector_opensearch7'\n",
    "\n",
    "docsearch = OpenSearchVectorSearch.from_texts(index_name = embedding_index_name,\n",
    "                                                  texts=chunks,\n",
    "                                       embedding=embeddings,\n",
    "                                       opensearch_url=os_domain_ep,\n",
    "                                       http_auth=(DOMAIN_ADMIN_UNAME, DOMAIN_ADMIN_PW)   )\n",
    "    \n",
    "print(\"docs inserted into opensearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf06c4",
   "metadata": {},
   "source": [
    "### Test OpenSearch vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_search_vector_store = OpenSearchVectorSearch(index_name=embedding_index_name,\n",
    "                                       embedding_function=embeddings,\n",
    "                                       opensearch_url=os_domain_ep,\n",
    "                                       http_auth=(DOMAIN_ADMIN_UNAME, DOMAIN_ADMIN_PW)   ) \n",
    "\n",
    "docs_ = open_search_vector_store.similarity_search(\"Data Warehousing costs\")\n",
    "print(\"opensearch results:\"+docs_[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacdeaa9",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ad791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import Dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import DynamoDBChatMessageHistory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        #response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        print(output.read().decode(\"utf-8\"))\n",
    "        return output.read().decode(\"utf-8\")\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "llm_endpoint_name = llm_endpoint_name\n",
    "\n",
    "llm=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=\"us-east-1\",\n",
    "        model_kwargs={\"temperature\": 1e-10},\n",
    "        content_handler=content_handler,\n",
    ")\n",
    "\n",
    "session_id = str(uuid4())\n",
    "chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=\"conversation-history-store\",\n",
    "        session_id=session_id\n",
    "    )\n",
    "\n",
    "messages = chat_memory.messages\n",
    "\n",
    "# Maintains immutable sessions\n",
    "# If previous session was present, create\n",
    "# a new session and copy messages, and \n",
    "# generate a new session_id \n",
    "if messages:\n",
    "    session_id = str(uuid4())\n",
    "    chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=\"conversation-history-store\",\n",
    "        session_id=session_id\n",
    "    )\n",
    "    # This is a workaround at the moment. Ideally, this should\n",
    "    # be added to the DynamoDBChatMessageHistory class\n",
    "    try:\n",
    "        messages = messages_to_dict(messages)\n",
    "        chat_memory.table.put_item(\n",
    "            Item={\"SessionId\": session_id, \"History\": messages}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "memory = ConversationBufferMemory(chat_memory=chat_memory, return_messages=True)\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=open_search_vector_store.as_retriever(),\n",
    "    memory = memory\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec088fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa.run(\"Data Warehousing costs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "chain = load_qa_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "chain({\"input_documents\": docs_, \"question\": \"Data Warehousing costs\"}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482f02b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
