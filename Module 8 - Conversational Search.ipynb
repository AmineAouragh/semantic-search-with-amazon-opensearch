{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d41ec6",
   "metadata": {},
   "source": [
    "# Conversational Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd860b-5997-45a6-869b-06793935c75d",
   "metadata": {},
   "source": [
    "In this lab, we leverage Lanchain framework to implement Retrieval Augmented Generation solution.For more informaiton about LangChain, please refere: https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03874004-bd68-4fac-a05a-3834986a8e89",
   "metadata": {},
   "source": [
    "## Step 1: Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34490d00-970a-49ac-97f5-1999ff4ca03f",
   "metadata": {},
   "source": [
    "Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06184986",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker \n",
    "!pip install opensearch-py\n",
    "!pip install wikipedia unstructured transformers\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e422af0-3961-4e99-9f90-f46d3cf78f0b",
   "metadata": {},
   "source": [
    "Initialize SageMaker, Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad970f25-32cf-4bd8-bfa4-f13e361a5b20",
   "metadata": {},
   "source": [
    "Get Cloud Formation stack output variables\n",
    "\n",
    "We also need to grab some key values from the infrastructure we provisioned using CloudFormation. To do this, we will list the outputs from the stack and store this in \"outputs\" to be used later.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = aws_region\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926ca92-3373-47a7-973e-3136ebfa2370",
   "metadata": {},
   "source": [
    "## Step 2 : Verify deployed endpoint for embedding and content generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49640d",
   "metadata": {},
   "source": [
    "### Get endpoint for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5962e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_endpoint_name=outputs['EmbeddingEndpointName']\n",
    "print(embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980c841-860a-46af-a3e2-2bf612c0134d",
   "metadata": {},
   "source": [
    "Verify embedding endpoint is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef415501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "\n",
    "describe_embedding_endpoint_response = sm_client.describe_endpoint(EndpointName=embedding_endpoint_name)\n",
    "\n",
    "while describe_embedding_endpoint_response[\"EndpointStatus\"] == 'Creating':\n",
    "    time.sleep(15)\n",
    "    print('.', end='')\n",
    "    describe_embedding_endpoint_response = sm_client.describe_endpoint(EndpointName=embedding_endpoint_name)\n",
    "print('enmbedding endpoint created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b48027",
   "metadata": {},
   "source": [
    "### Get endpoint for content generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecca035",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_endpoint_name=outputs['EmbeddingEndpointName']\n",
    "print(llm_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2214c-3487-45a8-babf-d4bf3b3ff021",
   "metadata": {},
   "source": [
    "Verify embedding endpoint is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d029d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "\n",
    "describe_llm_endpoint_response = sm_client.describe_endpoint(EndpointName=llm_endpoint_name)\n",
    "\n",
    "while describe_llm_endpoint_response[\"EndpointStatus\"] == 'Creating':\n",
    "    time.sleep(15)\n",
    "    print('.', end='')\n",
    "    describe_llm_endpoint_response = sm_client.describe_endpoint(EndpointName=llm_endpoint_name)\n",
    "print('LLM endpoint created')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747507b",
   "metadata": {},
   "source": [
    "## Test embedding endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "\n",
    "\n",
    "class TestContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        if len(embeddings) == 1:\n",
    "            return [embeddings[0]]\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "test_content_handler = TestContentHandler()\n",
    "\n",
    "test_embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=aws_region,\n",
    "    content_handler=test_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_embeddings.embed_documents([\"Hello World\"])[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa760ac7",
   "metadata": {},
   "source": [
    "## Test LLM endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86de9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "#method used to parse the inference model's response. we pass it as part of the model's config\n",
    "def parse_response_model(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return [gen[\"generated_text\"] for gen in model_predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to determine shard and data node counts for OpenSearch?\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": question,\n",
    "    \"parameters\":{\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 100,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": True,\n",
    "        \"temperature\": 0.9\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de27ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = query_endpoint_with_json_payload(\n",
    "    json.dumps(payload).encode(\"utf-8\"), endpoint_name=llm_endpoint_name\n",
    ")\n",
    "\n",
    "generated_texts = parse_response_model(query_response)\n",
    "\n",
    "print(f\"The generated output is: {generated_texts[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacdeaa9",
   "metadata": {},
   "source": [
    "## Step 3: Load documents with Langchain document loader and store vector into OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128529b-29fc-4e00-9faa-e5de2de70023",
   "metadata": {},
   "source": [
    "Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "Document loaders expose a \"load\" method for loading data as documents from a configured source. Here, we use `UnstructuredURLLoader` to load OpenSearch best practice web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0369d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "\n",
    "urls = [\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/bp.html\",\n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/sizing-domains.html\", \n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/petabyte-scale.html\",\n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/managedomains-dedicatedmasternodes.html\",\n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/cloudwatch-alarms.html\"]\n",
    "url_loader = UnstructuredURLLoader(urls=urls)\n",
    "url_texts = url_loader.load_and_split(text_splitter=text_splitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4d4df-fbb9-448a-9f43-f33fe7df9dd8",
   "metadata": {},
   "source": [
    "Somple example documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e53165-93fd-4434-a653-40ba1e98c67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_splits = url_texts\n",
    "all_splits[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57321a3a-db5c-4589-b6a4-6a007154d067",
   "metadata": {},
   "source": [
    "Create an OpenSearch cluster connection.\n",
    "Next, we'll use Python API to set up connection with Amazon Opensearch Service domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede13e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "auth = (\"master\",\"Semantic123!\")\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1763d0-cfee-49f6-80ab-c3ce403a525d",
   "metadata": {},
   "source": [
    "Use `OpenSearchVectorSearch` in LangChain to ingest vector into OpenSearch. You can specify more parameters to create kNN index with specified properties. Some parameters like:\n",
    "engine: “nmslib”, “faiss”, “lucene”; default: “nmslib”\n",
    "\n",
    "space_type: “l2”, “l1”, “cosinesimil”, “linf”, “innerproduct”; default: “l2”\n",
    "\n",
    "ef_search: Size of the dynamic list used during k-NN searches. Higher values lead to more accurate but slower searches; default: 512\n",
    "\n",
    "ef_construction: Size of the dynamic list used during k-NN graph creation. Higher values lead to more accurate graph but slower indexing speed; default: 512\n",
    "\n",
    "m: Number of bidirectional links created for each new element. Large impact on memory consumption. Between 2 and 100; default: 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a03874-5a51-456e-b36c-e31771d73ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Langchain embedding endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671fdfd-0632-49b8-b7f7-05493baa3ece",
   "metadata": {},
   "source": [
    "To build a simiplied QA application with LangChain, we need to wrap up our SageMaker endpoints for embedding model and LLM into `langchain.embeddings.SagemakerEndpointEmbeddings` and `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. That requires a overwrite methods of `SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding mdoel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Callable\n",
    "import json\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.schema import Document\n",
    "\n",
    "class BulkSagemakerEndpointEmbeddings(SagemakerEndpointEmbeddings):\n",
    "        def embed_documents(\n",
    "            self, texts: List[str], chunk_size: int = 5\n",
    "        ) -> List[List[float]]:\n",
    "            \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "            Args:\n",
    "                texts: The list of texts to embed.\n",
    "                chunk_size: The chunk size defines how many input texts will\n",
    "                    be grouped together as request. If None, will use the\n",
    "                    chunk size specified by the class.\n",
    "\n",
    "            Returns:\n",
    "                List of embeddings, one for each text.\n",
    "            \"\"\"\n",
    "            results = []\n",
    "            _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "            for i in range(0, len(texts), _chunk_size):\n",
    "                response = self._embedding_func(texts[i:i + _chunk_size])\n",
    "                results.extend(response)\n",
    "            return results\n",
    "        \n",
    "class EmbeddingContentHandler(EmbeddingsContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "\n",
    "            input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "            return input_str.encode('utf-8') \n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            embeddings = response_json[\"embedding\"]\n",
    "            if len(embeddings) == 1:\n",
    "                return [embeddings[0]]\n",
    "            return embeddings\n",
    "        \n",
    "embeddings = BulkSagemakerEndpointEmbeddings( \n",
    "            endpoint_name=embedding_endpoint_name,\n",
    "            region_name=aws_region, \n",
    "            content_handler=EmbeddingContentHandler())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d425b-0aa0-4e40-9be8-db082b3c5e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "os_domain_ep = 'https://'+aos_host\n",
    "\n",
    "embedding_index_name = 'opensearch_kb_vector'\n",
    "\n",
    "if len(all_splits) > 500:\n",
    "    for i in range(0, len(all_splits), 500):\n",
    "        start = i\n",
    "        end = i+500\n",
    "        if end > len(all_splits):\n",
    "            end = len(all_splits)-1\n",
    "        docs = all_splits[start:end]\n",
    "        OpenSearchVectorSearch.from_documents(\n",
    "            index_name = embedding_index_name,\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth\n",
    "        )\n",
    "        print(f\"ingest documents from {start} to {end}\", start, end)\n",
    "else:\n",
    "    OpenSearchVectorSearch.from_documents(\n",
    "            index_name = embedding_index_name,\n",
    "            documents=all_splits,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth\n",
    "        )\n",
    "    print(f\"ingest documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369f552-2b2c-49fe-955b-fd814183b720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=embedding_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0d9dd-0350-41ac-a844-2d64ecd11dce",
   "metadata": {},
   "source": [
    "When you use LangChain `OpenSearchVectorSearch` to store embedding with OpenSearch kNN index, you can specify parameters to choose different Appriximate Near Neighbour(ANN) algrithoms. For more information, please refer OpenSearch kNN documentaion: https://opensearch.org/docs/latest/search-plugins/knn/knn-index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610f4d3-1aa7-4a03-a668-90ebd4874d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customized_embedding_index_name = 'customized_opensearch_kb_vector'\n",
    "\n",
    "OpenSearchVectorSearch.from_documents(\n",
    "            index_name = customized_embedding_index_name,\n",
    "            documents=all_splits,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth,\n",
    "            engine=\"faiss\",\n",
    "            space_type=\"innerproduct\",\n",
    "            ef_construction=256,\n",
    "            m=48,\n",
    "        )\n",
    "print(f\"ingest documents into customized knn index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa8d63-6a13-42dd-9123-8d6ef19218a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=customized_embedding_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a108a-00f4-4e40-aea2-ea2e9091315a",
   "metadata": {},
   "source": [
    "### OpenSearch vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ecb4d-45ab-4cb7-8fa4-64cc7ef322a7",
   "metadata": {},
   "source": [
    "We can use `OpenSearchVectorSearch` for vector store or we can extend the class to define new fuction to calculate documents relevance score if you want to use relevance score to filter document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110d127-b60b-45e1-a178-37fd175537ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimiliarOpenSearchVectorSearch(OpenSearchVectorSearch):\n",
    "    \n",
    "    def relevance_score(self, distance: float) -> float:\n",
    "        return distance\n",
    "    \n",
    "    def _select_relevance_score_fn(self) -> Callable[[float], float]:\n",
    "        return self.relevance_score\n",
    "    \n",
    "\n",
    "open_search_vector_store = SimiliarOpenSearchVectorSearch(\n",
    "                                    index_name=embedding_index_name,\n",
    "                                    embedding_function=embeddings,\n",
    "                                    opensearch_url=os_domain_ep,\n",
    "                                    http_auth=auth\n",
    "                                    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92332ed7-546f-493c-8f42-705275bf70e7",
   "metadata": {},
   "source": [
    "Show the documents which are similiar with question \"How to determine shard and data node counts for OpenSearch?\". Be default, 4 documents are returned. You can specify \"k\" parameter. See the [doc](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html#langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.similarity_search_with_score) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ = open_search_vector_store.similarity_search_with_score(question, k=5)\n",
    "\n",
    "print(\"found document number:\" + str(len(docs_)))\n",
    "\n",
    "print(\"opensearch results:\\n\")\n",
    "for doc in docs_:\n",
    "    print(doc)\n",
    "    print(\"\\n-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "os_domain_ep = 'https://'+aos_host\n",
    "\n",
    "embedding_index_name = 'opensearch_best_practice_embedding'\n",
    "\n",
    "if len(all_splits) > 500:\n",
    "    for i in range(0, len(all_splits), 500):\n",
    "        start = i\n",
    "        end = i+500\n",
    "        if end > len(all_splits):\n",
    "            end = len(all_splits)-1\n",
    "        docs = all_splits[start:end]\n",
    "        OpenSearchVectorSearch.from_documents(\n",
    "            index_name = embedding_index_name,\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth\n",
    "        )\n",
    "        print(f\"ingest documents from {start} to {end}\", start, end)\n",
    "else:\n",
    "    OpenSearchVectorSearch.from_documents(\n",
    "            index_name = embedding_index_name,\n",
    "            documents=all_splits,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth\n",
    "        )\n",
    "    print(f\"ingest documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=embedding_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf094f03-01fb-4ab2-b747-164edda91365",
   "metadata": {},
   "source": [
    "You will see new index has different setting for vector field. For more information about OpenSearch engine, spacetype etc, please refer OpenSearch documentation: https://opensearch.org/docs/latest/search-plugins/knn/knn-index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864c097-bc9e-42c5-b02b-85a8df400436",
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_embedding_index_name = 'customized_opensearch_best_practice_embedding'\n",
    "\n",
    "OpenSearchVectorSearch.from_documents(\n",
    "            index_name = customized_embedding_index_name,\n",
    "            documents=all_splits,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth,\n",
    "            engine=\"faiss\",\n",
    "            space_type=\"innerproduct\",\n",
    "            ef_construction=256,\n",
    "            m=48,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56131ff2-ea2c-4cee-87c2-6400822bb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=customized_embedding_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f59d89-7580-428e-b784-716e287e4ebe",
   "metadata": {},
   "source": [
    "## Step 4: Test LLM without context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c089f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import Dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import DynamoDBChatMessageHistory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        print(\"Prompt Input:\\n\" + input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        print(\"LLM generated text:\\n\" + response_json[0][\"generated_text\"])\n",
    "        return response_json[0][\"generated_text\"]\n",
    "    \n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4247d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"max_length\": 4096,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 100,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.9\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb236e6-e242-4d1a-bce3-e2f00409035a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_hullucination=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898f7d3-289d-4701-99b5-3eb35eb1520a",
   "metadata": {},
   "source": [
    "To better illustrate why we need retrieval-augmented generation (RAG) based approach to solve the question and anwering problem. Let's directly ask the model a question and see how they respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa531974-2eae-4d68-92c0-0a474a80a21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "llm_hullucination(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccaba14-45f9-43f5-865f-77965e18e7b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Generated answer is not fully accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18229ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_40b_endpoint_name = 'RAG-LLM-huggingface-llm-falcon-40b-inst-2023-08-29-08-14-50-272'\n",
    "\n",
    "llm_40b_hullucination=SagemakerEndpoint(\n",
    "        endpoint_name=llm_40b_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "llm_40b_hullucination(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0e71e-bf17-4f6e-aaff-be56a874c212",
   "metadata": {},
   "source": [
    "## Step 5: Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383f6de-785c-4b68-9e47-a73cdffbd9b5",
   "metadata": {
    "tags": []
   },
   "source": [
    " ### Langchain retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac62f57-0f57-4591-95b8-0f2f04775966",
   "metadata": {},
   "source": [
    "Here we use OpenSearch vector store as retriever to get similiar documents with query. We can also specify similarity scrore threshhold to return high relevant documents. Use \"k\" to limit how many documents to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = open_search_vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        'k': 5,\n",
    "        'score_threshold': 0.62\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda6fd8-7dbf-4c7d-86bb-95b3005fc024",
   "metadata": {},
   "source": [
    "### LLM without Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db375107-9d31-4e35-acda-a5d20941ca7a",
   "metadata": {},
   "source": [
    "To avoid LLM generate non factual answer, we can specify low \"temperature\" value when calling LLM to generate conent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29542ec7-0296-4f3e-b89d-fe9064665960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"max_length\": 4096,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 100,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.001\n",
    "        }\n",
    "\n",
    "llm=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\" #stuff, refine, map_reduce, and map_rerank\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869cfe8-963f-4053-aa00-22f7ff76a132",
   "metadata": {},
   "source": [
    "Compare the content generated with RAG and LLM without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feaca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "result = qa({\"query\": question})\n",
    "\n",
    "print(\"result:\" + result[\"result\"])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774aac71-7220-4205-bf2d-1ed253dd5b1e",
   "metadata": {},
   "source": [
    "### Use customized prompt for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479f506-bae2-4573-8126-7b2fa6b42182",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can also customized prompt per your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following \"Context:\" to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. \n",
    "Context:{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template=template)\n",
    "\n",
    "qa_with_prompt = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8517d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "result = qa_with_prompt({\"query\": question})\n",
    "\n",
    "print(\"\\n### Generated result:\" + result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c36dca-ae3d-4aa3-8f60-ed66dffaaba4",
   "metadata": {},
   "source": [
    "### Try to ask some questions which are not covered in your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2d76b-9157-4803-9438-955d12bfd700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = qa_with_prompt({\"query\": \"Who is Jianwei?\"})\n",
    "\n",
    "print(\"\\n### Generated result:\" + result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5710a4f-5652-4a2d-a126-0ac8e10aceef",
   "metadata": {},
   "source": [
    "### Return source documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1054077-8a3f-40e1-a5e8-5ef65cbb11c1",
   "metadata": {},
   "source": [
    "You can also return the source documents to help you find original knoledge base document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba727f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_source = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"refine\" #stuff, refine, map_reduce, and map_rerank\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "result = qa_with_source({\"query\": question})\n",
    "\n",
    "print(\"result:\" + result[\"result\"])\n",
    "print(\"\\n\\n===========================\")\n",
    "print(\"\\nsource documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc)\n",
    "    print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f20ca-0787-41fa-9f2c-8ad2887e2640",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For `RetrievalQA`, you have 4 methods to use retrieved documents as content, stuff, refine, map_reduce, and map_rerank. Please refere https://python.langchain.com/docs/modules/chains/document/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c88750-e300-45f9-988f-3e238e72f84f",
   "metadata": {},
   "source": [
    "## Step 6: Conversational search by memorizing the history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d64dc6-df81-4df2-806a-14cebec9b6ce",
   "metadata": {},
   "source": [
    "### Langchain Memory with DynamoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824f779-5233-4047-a26a-38e035354445",
   "metadata": {},
   "source": [
    "Here we create new session and use DynamoDB as backend to store history conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddb_table_name = outputs['DynamoDBTableName']\n",
    "session_id = str(uuid4())\n",
    "chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=ddb_table_name,\n",
    "        session_id=session_id\n",
    "    )\n",
    "\n",
    "messages = chat_memory.messages\n",
    "\n",
    "# Maintains immutable sessions\n",
    "# If previous session was present, create\n",
    "# a new session and copy messages, and \n",
    "# generate a new session_id \n",
    "if messages:\n",
    "    session_id = str(uuid4())\n",
    "    chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=\"conversation-history-store\",\n",
    "        session_id=session_id\n",
    "    )\n",
    "    # This is a workaround at the moment. Ideally, this should\n",
    "    # be added to the DynamoDBChatMessageHistory class\n",
    "    try:\n",
    "        messages = messages_to_dict(messages)\n",
    "        chat_memory.table.put_item(\n",
    "            Item={\"SessionId\": session_id, \"History\": messages}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "memory = ConversationBufferMemory(chat_memory=chat_memory, return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_memory = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory = memory,\n",
    "    chain_type=\"stuff\" #stuff, refine, map_reduce, and map_rerank\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1181e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_with_memory({\"query\": question})\n",
    "print(\"result:\" + result[\"result\"])\n",
    "print(\"\\nHistory:\\n===========================\")\n",
    "for x in range(0,len(result[\"history\"]),2):\n",
    "    print(\"Question:\")\n",
    "    print(result[\"history\"][x])\n",
    "    print(\"Answer:\")\n",
    "    print(result[\"history\"][x+1])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465b41e-7b55-4dff-92af-22eb9ae0259c",
   "metadata": {},
   "source": [
    "Try to ask one more question, the history conversation stored in DynamoDB are also used as context to LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14072a-dffc-417b-b84a-f46e4a807eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_following_question = 'if my data growth is very fast'\n",
    "result = qa_with_memory({\"query\": second_following_question})\n",
    "print(\"result:\" + result[\"result\"])\n",
    "print(\"\\nHistory:\\n===========================\")\n",
    "for x in range(0,len(result[\"history\"]),2):\n",
    "    print(\"Question:\")\n",
    "    print(result[\"history\"][x])\n",
    "    print(\"Answer:\")\n",
    "    print(result[\"history\"][x+1])\n",
    "    print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad24015-3667-470b-bc79-845c8cb15c6e",
   "metadata": {},
   "source": [
    "# Deploy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67a61b-dbdb-47ec-a063-d4ed4b824e88",
   "metadata": {},
   "source": [
    "## Deploy embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id, embedding_model_version = (\n",
    "    \"huggingface-textembedding-gpt-j-6b-fp16\",\n",
    "    \"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc250041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef212514",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_endpoint_name = name_from_base(f\"RAG-embedding-{embedding_model_id}\")\n",
    "\n",
    "embedding_instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "\n",
    "embedding_deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=embedding_model_id,\n",
    "    model_version=embedding_model_version,\n",
    "    instance_type=embedding_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "embedding_model_uri = model_uris.retrieve(\n",
    "    model_id=embedding_model_id, model_version=embedding_model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "embedding_model = Model(\n",
    "    image_uri=embedding_deploy_image_uri,\n",
    "    model_data=embedding_model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=embedding_endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "embedding_model_predictor = embedding_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=embedding_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca867643-e25d-46dc-b33e-2a4898f45e5b",
   "metadata": {},
   "source": [
    "## Deploy content generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_id, llm_model_version, = (\n",
    "    \"huggingface-llm-falcon-7b-instruct-bf16\",\n",
    "    \"*\",\n",
    ")\n",
    "\n",
    "llm_model_ids = ['huggingface-llm-falcon-40b-bf16',\n",
    "             'huggingface-llm-falcon-40b-instruct-bf16',\n",
    "             'huggingface-llm-falcon-7b-bf16',\n",
    "             'huggingface-llm-falcon-7b-instruct-bf16']\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = Dropdown(\n",
    "    options=llm_model_ids,\n",
    "    value=llm_model_id,\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c243cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_id = model_dropdown.value\n",
    "print(llm_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_endpoint_name = name_from_base(f\"RAG-LLM-{llm_model_id}\")\n",
    "\n",
    "llm_inference_instance_type = \"ml.g5.48xlarge\"\n",
    "\n",
    "health_check_timeout = 1800\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff14a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "llm_model = JumpStartModel(model_id=llm_model_id, instance_type=llm_inference_instance_type)\n",
    "llm_model.env['SM_NUM_GPUS'] = '8'\n",
    "llm_model.env['MAX_INPUT_LENGTH'] = '2048'\n",
    "llm_model.env['MAX_TOTAL_TOKENS'] = '4096'\n",
    "llm_model_predictor = llm_model.deploy(\n",
    "    endpoint_name=llm_endpoint_name,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776fb36-fca2-46cf-9aa2-56a09f178cd2",
   "metadata": {},
   "source": [
    "## deploy with more code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c92f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_endpoint_name = name_from_base(f\"RAG-LLM-{llm_model_id}\")\n",
    "\n",
    "llm_deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=llm_model_id,\n",
    "    model_version=llm_model_version,\n",
    "    instance_type=llm_inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "llm_model_uri = model_uris.retrieve(\n",
    "    model_id=llm_model_id, model_version=llm_model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "number_of_gpu = 8\n",
    "max_input_length = 2048\n",
    "max_total_tokens = 4096\n",
    "\n",
    "model_env = {\n",
    "    'HF_MODEL_ID': \"tiiuae/falcon-40b-instruct\",\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "    'MAX_INPUT_LENGTH': json.dumps(max_input_length),\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(max_total_tokens),\n",
    "}\n",
    "\n",
    "\n",
    "llm_model = Model(\n",
    "    image_uri=llm_deploy_image_uri,\n",
    "    model_data=llm_model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=llm_endpoint_name,\n",
    "    env=model_env\n",
    ")\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "llm_model_predictor = llm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=llm_inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=llm_endpoint_name,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    wait=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
